(window.webpackJsonp=window.webpackJsonp||[]).push([[43],{170:function(e,n,t){"use strict";var o={props:["title","published","updated","archived"],head:function(){return{title:this.title}}},r=t(8),component=Object(r.a)(o,(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",[t("h1",{staticClass:"title",attrs:{itemprop:"name headline"}},[e._v(e._s(e.title))]),e._v(" "),t("div",{staticClass:"time"},[e._v("Published: "),t("time",{attrs:{itemprop:"datePublished"}},[e._v(e._s(e.published))])]),e._v(" "),e.updated?t("div",{staticClass:"time"},[e._v("Updated: "),t("time",{attrs:{itemprop:"dateModified"}},[e._v(e._s(e.updated))])]):e._e(),e._v(" "),e.archived?t("div",{staticClass:"block"},[e._v("Note: This is an archived post. Information may not be relevant now.")]):e._e()])}),[],!1,null,null,null);n.a=component.exports},171:function(e,n,t){var content=t(173);"string"==typeof content&&(content=[[e.i,content,""]]),content.locals&&(e.exports=content.locals);(0,t(27).default)("041c60ae",content,!0,{sourceMap:!1})},172:function(e,n,t){"use strict";t(171)},173:function(e,n,t){var o=t(26)(!1);o.push([e.i,"img[data-v-f0c4a2de]{display:block;margin-bottom:.5em}",""]),e.exports=o},214:function(e,n,t){"use strict";t.r(n);var o=t(170),r={mixins:[o.a],components:{BlogTitle:o.a}},d=(t(172),t(8)),component=Object(d.a)(r,(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("article",{attrs:{itemprop:"blogPost",itemscope:"",itemtype:"https://schema.org/BlogPosting"}},[t("BlogTitle",{attrs:{title:"MPEG and JPEG Compression",published:"30 Jun 2014",archived:"true"}}),e._v(" "),t("div",{directives:[{name:"highlight",rawName:"v-highlight"}],staticClass:"content",attrs:{itemprop:"articleBody"}},[e._m(0),e._v(" "),t("div",{attrs:{name:"DataCompression","data-unique":"DataCompression"}}),e._v(" "),t("h2",[e._v("Data Compression")]),e._v(" "),t("p",[e._v("\n        The hunger for information keeps growing in the modern age. Science and\n        technology has not only made it possible to know what is happening on\n        the other end of the world but also witness it instantaneously.\n        Multimedia has been the most preferred means for sharing as it is the\n        richest form of information. Pictures and videos are more appealing than\n        plain text and convey a lot more information. Television broadcasting,\n        webpages, social networks, emails and multimedia messaging are some of\n        the popular ways for such communication. But there’s a price to pay for\n        this richness in information – huge storage requirements and larger\n        bandwidth for transmission. This cost can be reduced by the use of\n        data compression.\n      ")]),e._v(" "),e._m(1),e._v(" "),t("div",{attrs:{name:"LossyandLosselessCompression","data-unique":"LossyandLosselessCompression"}}),e._v(" "),t("h3",[e._v("Lossy and Losseless Compression")]),e._v(" "),t("p",[e._v("\n        There are basically two types of data compression – lossless and lossy\n        compression. Lossless compression is the kind in which samples of data\n        are ignored only when they have been represented at least once. Only\n        redundancy is removed but the number of data samples that occur remain\n        same. In contrast, in lossy compression, non-redundant information may\n        also be removed if they don’t produce significant alteration in the\n        perception of wholesome data. Decompression of entity compressed with\n        lossless compression gives us the exact original object whereas the\n        decompression of entity compressed with lossy compression gives us a\n        close estimate of the original object.\n      ")]),e._v(" "),t("p"),e._v(" "),t("hr"),e._v(" "),t("p",[e._v("\n        The level and nature of compression of data depends upon the various\n        data transformation and encoding algorithms. Such techniques rely on the\n        assumption that individual components of information (e.g. pixels on an\n        image) display a certain level of correlation. This correlation of\n        individual units may be exploited for the compression of the whole\n        entity. Some of these techniques have been explained below:\n      ")]),e._v(" "),t("div",{attrs:{name:"DiscreteCosineTransform","data-unique":"DiscreteCosineTransform"}}),e._v(" "),t("h3",[e._v("Discrete Cosine Transform")]),e._v(" "),t("p",[e._v("\n        DCT (Discrete Cosine Transform), like other\n        transformation techniques tries to decorrelate the basic units of data.\n        Correlated units mean the similar units spread over the same plane in\n        space. In case of images, it reduces or sometimes eliminates interpixel\n        redundancy. DCT is used to map this spatial\n        variation into uncorrelated frequency variation.\n        DCT is a linear function. Also, it is\n        invertible and its inverse gives the same original spectrum, therefore,\n        DCT itself is a lossless transformation but it\n        is usually mixed with lossy algorithms like quantization in it\n        applications like JPEG,\n        MPEG, etc.\n      ")]),e._v(" "),t("p",[e._v("\n        The several occurrences of the data points is expressed in terms of the\n        sum of cosine functions at different periodic frequencies. Cosine\n        functions are chosen over sine functions because of their advantage of\n        efficiency. DCT is quite similar to\n        DFT (Discrete Cosine Transform) but the only\n        difference is that only real numbers are taken into account in\n        DCT whereas\n        DFT takes in complex numbers too.\n      ")]),e._v(" "),e._m(2),e._v(" "),t("div",{attrs:{name:"DCT-I","data-unique":"DCT-I"}}),e._v(" "),t("h3",[e._v("DCT-I")]),e._v(" "),e._m(3),e._v(" "),t("div",{attrs:{name:"DCT-II","data-unique":"DCT-II"}}),e._v(" "),t("h3",[e._v("DCT-II")]),e._v(" "),e._m(4),e._v(" "),t("p",[e._v("\n        DCT-II is the most\n        widely used form of cosine transformation and\n        DCT in general means\n        DCT-II.\n        DCT-II, sometimes\n        also DCT-I is used for compression of\n        correlated pixels in JPEG,\n        MJPEG and\n        MPEG standards. The two dimensional\n        block-based DCT of 8×8 matrix is used for\n        encoding blocks of video. This encoding standard is defined by\n        IEEE 1180 to increase accuracy and reduce\n        mismatch errors.\n      ")]),e._v(" "),e._m(5),e._v(" "),t("div",{attrs:{name:"DCT-III","data-unique":"DCT-III"}}),e._v(" "),t("h3",[e._v("DCT-III")]),e._v(" "),e._m(6),e._v(" "),e._m(7),e._v(" "),t("div",{attrs:{name:"DCT-IV","data-unique":"DCT-IV"}}),e._v(" "),t("h3",[e._v("DCT-IV")]),e._v(" "),e._m(8),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Image and its DCT coefficients",src:"/media/mpeg/dct-diff1.png"}}),e._v(" "),t("b",[e._v("Figure 1.1 : Image and its\n          DCT coefficients")])]),e._v("\n      The coefficients retrieved i.e. X"),t("sub",[e._v("k")]),e._v(" represents the contribution\n      of combination of the spatial repetitions in two horizontal and vertical\n      dimensions of the picture block. DC (Discrete\n      Cosine) Coefficient is the coefficient relating to zero frequency indices\n      on both the dimensions. Quantization and Variable Length Coding follow\n      DCT to reduce further number of bits.\n      "),t("p"),e._v(" "),t("div",{attrs:{name:"Quantization","data-unique":"Quantization"}}),e._v(" "),t("h3",[e._v("Quantization")]),e._v(" "),t("p",[e._v("\n        Quantization is the process of converting a continuous range of\n        infinitely many values into a finite discrete set of all possible\n        values. The quantization process generally approximates the input set\n        into preferably smaller set. The advantage of quantization is that it\n        reduces the number of bits required for storing and transmitting the\n        data. Quantization can be of two types – scalar quantization and\n        vector quantization.\n      ")]),e._v(" "),t("div",{attrs:{name:"ScalarQuantization","data-unique":"ScalarQuantization"}}),e._v(" "),t("h3",[e._v("Scalar Quantization")]),e._v(" "),e._m(9),e._v(" "),t("div",{attrs:{name:"VectorQuantization","data-unique":"VectorQuantization"}}),e._v(" "),t("h3",[e._v("Vector Quantization")]),e._v(" "),t("p",[e._v("\n        Vector quantization is also known as ‘pattern matching quantization’ or\n        ‘block quantization’. It involves grouping together of input symbols\n        into single units. Grouping enhances optimality of the quantizer but\n        also consumes higher computational resources than scalar quantization. A\n        centroid point is used to indicate a single group or cluster. This kind\n        of quantization is used for lossy data compression of digital data,\n        density estimation and signal processing because it is powerful for\n        detecting density of huge data of high dimensions. For values in vector\n        space of multi-dimensions, quantization encodes them to a bounded set of\n        values from a lower dimensional subspace of discrete nature.\n      ")]),e._v(" "),t("div",{attrs:{name:"QuantizationofDigitalImage","data-unique":"QuantizationofDigitalImage"}}),e._v(" "),t("h3",[e._v("Quantization of Digital Image")]),e._v(" "),e._m(10),e._v(" "),e._m(11),e._v(" "),e._m(12),e._v(" "),e._m(13),e._v(" "),e._m(14),e._v(" "),e._m(15),e._v(" "),e._m(16),e._v(" "),t("p",[e._v("\n        Now dividing each elements from M matrix with corresponding elements\n        (from same row i and same column j) from Q matrix like\n      ")]),e._v(" "),e._m(17),e._v(" "),e._m(18),e._v(" "),t("p"),e._v(" "),t("p",[e._v("\n        The matrix thus obtained is processed through other techniques like\n        zigzag scanning, entropy encoding etc.\n      ")]),e._v(" "),t("div",{attrs:{name:"EntropyCoding","data-unique":"EntropyCoding"}}),e._v(" "),t("h3",[e._v("Entropy Coding")]),e._v(" "),t("p",[e._v("\n        Entropy literally refers to the lack of predictability or order. In the\n        context of information theory and engineering, entropy is the\n        measurement of the uncertainty of a variable. Entropy also gives the\n        measure of similarity of dispersion of basic units. Lower is the\n        entropy, higher is the compression. Techniques like statistical\n        forecasting can reduce entropy.\n      ")]),e._v(" "),t("p",[e._v("\n        Entropy encoding is the process of ordering the input elements in an\n        optimal way and assigning code for each of them. It is a lossless\n        compression. Entropy encoding can be broken into two steps. The first\n        step is Zigzag scanning and the second one is Variable Length Coding\n        (VLC). The second one has major contribution\n        to entropy encoding.\n      ")]),e._v(" "),t("div",{attrs:{name:"ZigzagScanordering","data-unique":"ZigzagScanordering"}}),e._v(" "),t("h3",[e._v("Zigzag Scan ordering")]),e._v(" "),t("p",[e._v("\n        The most AC values in a quantized matrix are\n        zero. Zigzag scan can be used to gather more number of zeros together.\n        Bringing zeroes together increases the optimality of the encoding\n        process that follows. Zigzag scanning groups the low frequency\n        coefficients before the high frequency coefficients. This processes\n        serializes the matrix into a string.\n      ")]),e._v(" "),t("p",[e._v("\n        The 8 x 8 matrix is mapped into a one-dimensional array of 1 x 64\n        vector. Grouping zeros enables us to encode the bitstream in the pair of\n        ‘skip and value’. Here, ‘skip’ means the number of occurrences of zero\n        and ‘value’ means the occurrence of next non-zero component in\n        the sequence.\n      ")]),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Zigzag Scan",src:"/media/mpeg/zigzag.jpg"}}),e._v(" "),t("br"),e._v(" "),t("b",[e._v("Figure 1.2 Zigzag Scan")])]),e._v("\n      In the above diagram, the first index is zero, so the first row is row 0\n      and the last row is row 7, similar with the columns. The scanning proceeds\n      like this – DC, AC01,\n      AC10, AC20,\n      AC11, … , AC57,\n      AC67, AC76,\n      AC77.\n      "),t("p"),e._v(" "),t("center",[t("img",{attrs:{alt:"Serialization of coefficients with zigzag scanning",src:"/media/mpeg/zigzag2.jpg"}}),t("br"),e._v(" "),t("b",[e._v("Figure 1.3: Serialization of coefficients with zigzag scanning")])]),e._v(" "),t("p"),e._v(" "),t("div",{attrs:{name:"VariableLengthCoding(VLC)","data-unique":"VariableLengthCoding(VLC)"}}),e._v(" "),t("h3",[e._v("Variable Length Coding (VLC)")]),e._v(" "),e._m(19),e._v(" "),t("div",{attrs:{name:"HuffmanCoding","data-unique":"HuffmanCoding"}}),e._v(" "),t("h4",[e._v("Huffman Coding")]),e._v(" "),t("p",[e._v("\n        Huffman Coding like other coding techniques for compression is a\n        statistical technique that attempts to reduce the number of bits\n        required to represent a string. The basic idea of this coding technique\n        is to assign a shorted code for the most frequent symbols. Huffman\n        coding algorithm is a greedy process. This technique was introduced by\n        David Huffman (1925-1999) in 1952.The vital part of this coding\n        technique is to generate the codes which are called Huffman codes. Code\n        Book is used to store the Huffman codes. Huffman algorithm is a\n        bottom-up approach.\n      ")]),e._v(" "),e._m(20),e._v(" "),e._m(21),e._v(" "),t("p",[e._v("\n        Thus generated binary tree is unambiguous for decomposing it during\n        decoding process is only possible in exactly one way. Since leaf nodes\n        are used to store symbols, no code happens to be the prefix of\n        another code.\n      ")]),e._v(" "),t("p"),e._v(" "),t("pre",{staticClass:"language-art"},[e._v("       *\n      / \\\n     (0)(1)\n      /  \\\n    (10)(11)\n      / \\\n    (110)(111)\n")]),e._v(" "),t("p"),e._v(" "),t("p"),e._v(" "),t("center",[t("b",[e._v("Figure 1.4: An Example of Huffman Tree")])]),e._v(" "),t("p"),e._v(" "),t("div",{attrs:{name:"Context-AdaptiveBinaryArithmeticCoding(CABAC)andContext-adaptiveVariable-lengthCoding(CAVLC)","data-unique":"Context-AdaptiveBinaryArithmeticCoding(CABAC)andContext-adaptiveVariable-lengthCoding(CAVLC)"}}),e._v(" "),e._m(22),e._v(" "),t("p",[e._v("\n        CABAC and CAVLC are\n        coding algorithms used to encode the syntax elements when theprobability\n        of its occurrence in the context is given. These are one of the forms of\n        entropy encoding and are lossless. CABAC has\n        higher compression ratio than CAVLC but\n        decoding a CABAC encoded entity requires more\n        processing than decoding a CAVLC encoded\n        entity. Parallelization and vectorization in\n        CABAC is more difficult than in\n        CAVLC.\n      ")]),e._v(" "),t("div",{attrs:{name:"Exponential-GolombCoding","data-unique":"Exponential-GolombCoding"}}),e._v(" "),t("h4",[e._v("Exponential-Golomb Coding")]),e._v(" "),t("p",[e._v("\n        Exponential-Golomb (Exp-Golomb) coding is an alternative for\n        CABAC and\n        CAVLC which provides a more simple yet better\n        structured VLC technique for encoding syntax\n        elements. A non-integer code is used to parameterize the encoding in\n        this process.\n      ")]),e._v(" "),t("div",{attrs:{name:"WaveletTransformation","data-unique":"WaveletTransformation"}}),e._v(" "),t("h3",[e._v("Wavelet Transformation")]),e._v(" "),t("p",[e._v("\n        Wavelet transformation is a technique for compression of data used in\n        images and sometimes in audio and video. It is used as a substitute to\n        DCT for transformation of coefficients from\n        spatial domain to frequency domain. Wavelet compression can either be\n        lossy or lossless whereas DCT is always\n        lossless. This is why wavelet may provide more compression ratio than\n        DCT.\n      ")]),e._v(" "),t("div",{attrs:{name:"RunlengthEncoding(RLE)","data-unique":"RunlengthEncoding(RLE)"}}),e._v(" "),t("h3",[e._v("Run length Encoding (RLE)")]),e._v(" "),t("p",[e._v("\n        un Length Encoding is a technique of encoding data where consecutively\n        occurring entities are represented only once with a symbol along with\n        the frequency. The original sequence is transformed into a smaller run\n        with data values and their count thus enabling compression.\n        RLE is considered to be one of the oldest data\n        compression approaches. This approach is suitable for all kinds of\n        information, text or binary. RLE may not be\n        suitable for data which do not have items repeating consecutively.\n        RLE can be performed in bit-level, byte-level\n        or even pixel-level in case of images. RLE is\n        a lossless compression.\n      ")]),e._v(" "),e._m(23),e._v(" "),e._m(24),e._v(" "),t("div",{attrs:{name:"Chromasubsampling","data-unique":"Chromasubsampling"}}),e._v(" "),t("h3",[e._v("Chroma subsampling")]),e._v(" "),e._m(25),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Color Subsampling",src:"/media/mpeg/9.jpg"}}),t("br"),e._v(" "),t("b",[e._v("Figure 1.5: Demonstration of color subsampling")])]),e._v("\n      The RGB color model uses the basic Red (R),\n      Green (G) and Blue (B) colors to get wide range of other colors by adding\n      them up, and therefore, RGB color space is\n      called additive color space. The RGB color space\n      can also be transformed to luminance-chrominance. The advantage of\n      expressing them independently into the components luma and chroma is that\n      different bandwidth can be assigned for each component thus\n      promoting compression.\n      "),t("p"),e._v(" "),e._m(26),e._v(" "),t("p",[e._v("Y’ = 0.212671 * R’ + 0.715160 * G’ + 0.072169 * B’")]),e._v(" "),e._m(27),e._v(" "),e._m(28),e._v(" "),e._m(29),e._v(" "),e._m(30),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Components in different sub-sample rations",src:"/media/mpeg/subsample_ratios.jpg"}}),t("br"),e._v(" "),t("b",[e._v("Figure 1.6: Components in different sub-sample ratios")])]),e._v("\n      Subsampled videos are always required to be stored using even dimensions\n      in order to prevent ‘ghosts’ effect due to chroma mismatch. This occurs as\n      shadows are seen because some colors appear to be ahead of or behind the\n      rest of the image frame.\n      "),t("p"),e._v(" "),e._m(31),e._v(" "),t("hr"),e._v(" "),t("div",{attrs:{name:"JPEG","data-unique":"JPEG"}}),e._v(" "),t("h2",[e._v("JPEG")]),e._v(" "),t("div",{attrs:{name:"Introduction","data-unique":"Introduction"}}),e._v(" "),t("h3",[e._v("Introduction")]),e._v(" "),e._m(32),e._v(" "),t("p",[e._v("\n        JPEG is the most widely used and a very\n        flexible digital photograph compression standard. It is the best example\n        of continuous tone image compression technique. Continuous tone is the\n        term used for images in which each color at any point is independent\n        color or single tone from its surrounding points and the quality of the\n        image depends on how many points are there in given size of image.\n        JPEG is lossy image compression method where\n        the size of image and its quality is inversely proportional.\n        JPEG compression technique can withstand\n        compression ratio of 10:1 without compromising with quality noticeably.\n        There is an alternative standard for image compression which is not used\n        widely and not compatible across variety of devices.\n      ")]),e._v(" "),e._m(33),e._v(" "),t("div",{attrs:{name:"JPEGStandardizationBody","data-unique":"JPEGStandardizationBody"}}),e._v(" "),t("h3",[e._v("JPEG Standardization Body")]),e._v(" "),e._m(34),e._v(" "),t("div",{attrs:{name:"HistoryofJPEG","data-unique":"HistoryofJPEG"}}),e._v(" "),t("h3",[e._v("History of JPEG")]),e._v(" "),t("p",[e._v("\n        The first JPEG standard –\n        JPEG Part 1 was published in 1992, which\n        explained the requirements and guidelines for digital compression of\n        images. JPEG Part 2 released on 1994 explained\n        compliance testing and JPEG Part 3 released on\n        1996 explained the extensions. JPEG Part 4\n        released on 1998 added profiles, index tags, color spaces, markers and\n        other designations to JPEG.\n        JPEG Part 5 is under development and in a\n        recent press release, the joint group has revealed that\n        JFIF will be standardized and designated as\n        JPEG Part 5.\n      ")]),e._v(" "),e._m(35),e._v(" "),e._m(36),e._v(" "),t("div",{attrs:{name:"CompressioninJPEGStandard","data-unique":"CompressioninJPEGStandard"}}),e._v(" "),t("h3",[e._v("Compression in JPEG Standard")]),e._v(" "),t("div",{attrs:{name:"PsychovisualCompression","data-unique":"PsychovisualCompression"}}),e._v(" "),t("h3",[e._v("Psychovisual Compression")]),e._v(" "),t("p",[e._v("\n        The human perception is not highly sensitive to detailed spatial\n        information. This means our vision has limited response to details\n        around edges of objects and shot-changes. This nature of human\n        perception can be exploited to reduce the amount of information in an\n        image or video frame and thus allows some considerable amount of\n        compression without real significant notice.\n      ")]),e._v(" "),e._m(37),e._v(" "),t("div",{attrs:{name:"SpatialRedundancyRemoval","data-unique":"SpatialRedundancyRemoval"}}),e._v(" "),t("h3",[e._v("Spatial Redundancy Removal")]),e._v(" "),e._m(38),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Transformation of 8 x 8 picture matrix to DCT coefficients",src:"/media/mpeg/8x8.jpg"}}),e._v(" "),t("b",[e._v("Figure 2.1 : Transformation of 8 x 8 picture matrix to\n          DCT coefficients")])]),e._v("\n      The above example shows transformation of 8 x 8 matrix. The transformed\n      matrix is known as matrix of DCT coefficients.\n      DCT coefficients are of two kinds –\n      DC (Direct Current) coefficients and\n      AC (Alternate Current) coefficients.\n      DC coefficient is the one which has zero\n      frequency across both the horizontal and vertical dimension. A 8 x 8\n      matrix of DCT coefficients has 64 elements, one\n      of them is DC coefficient and the rest 63 are\n      AC coefficients. In the above example, 239 (\n      element from first row and first column of the matrix of\n      DCT coefficients) is the\n      DC coefficient. The rest of the elements in the\n      matrix are AC coefficients.\n      "),t("p"),e._v(" "),t("p",[e._v("\n        Wavelet Transformation may also be employed as a substitute to\n        DCT, although it is more widely used in the\n        JPEG 2000 standard.\n        DCT is always lossless whereas wavelet may be\n        lossy. Quantization is performed over the retrieved transformation\n        coefficients to convert higher frequency coefficients into zero. Some\n        insignificant amount of information is lost during this process. The\n        quantized matrix then entropy coded. The\n        DC component of the matrix is encoded using\n        DPCM (Differential Pulse-Code Modulation)\n        which is a derivation of the PCM (Pulse Code\n        Modulation) technique. DPCM uses the\n        differential value among the sample nodes for coding. Whereas for the\n        AC coefficients, zigzag scanning is done to\n        align zeroes on one corner. Since zeroes are accumulated at the end, End\n        of Block (EOB) marker can be used to replace\n        the final run of zeroes, which further reduces the bit-size. Run length\n        Encoding (RLE) is applied to reduce the\n        redundancy. Then, Huffman Coding or Arithmetic coding is to be done to\n        get smaller array of data. Although\n        JPEG standard recommends any of these two\n        Variable-Length Coding (VLC) to be used,\n        Huffman Coding is preferred for its better compression.\n        VLC is the last step during encoding and first\n        step during decoding of JPEG images.\n      ")]),e._v(" "),t("div",{attrs:{name:"DecodingofJPEG","data-unique":"DecodingofJPEG"}}),e._v(" "),t("h3",[e._v("Decoding of JPEG")]),e._v(" "),t("p",[e._v("\n        Decoding of JPEG file is the inverse process\n        of its encoding. The decoding process starts with entropy decoding.\n        Variable-Length decoding is done. Huffman Code Book is used for\n        identifying the symbols for which the codes are used. Inverse\n        quantization and then Inverse DCT (which is\n        DCT-III) further\n        decodes the image file. This gives us a very very close match of the raw\n        image that was fed to the encoder. The decoding process requires\n        information about how the encoding or the compression has been done.\n        This information is available within the\n        JPEG file as\n        CH (Compression History).\n      ")]),e._v(" "),t("div",{attrs:{name:"JPEGCompressionModes","data-unique":"JPEGCompressionModes"}}),e._v(" "),t("h3",[e._v("JPEG Compression Modes")]),e._v(" "),t("p",[e._v("\n        There are four modes of JPEG compression:\n      ")]),e._v(" "),t("div",{attrs:{name:"LosslessJPEG","data-unique":"LosslessJPEG"}}),e._v(" "),t("h3",[e._v("Lossless JPEG")]),e._v(" "),t("p",[e._v("\n        Lossless JPEG compression mode uses only\n        predictive technique. No information is discarded if it isn’t redundant.\n        Entropy coding is employed for this no quantization is done. Eight kinds\n        of prediction schemes are used.\n      ")]),e._v(" "),t("div",{attrs:{name:"BaselineorSequentialJPEG","data-unique":"BaselineorSequentialJPEG"}}),e._v(" "),t("h3",[e._v("Baseline or Sequential JPEG")]),e._v(" "),t("p",[e._v("\n        This is the most common mode of\n        JPEG compression. It employs all the lossy and\n        lossless compression algorithms for psychovisual compression as well as\n        spatial prediction. It involves chroma subsampling,\n        DCT, quantization and entropy encoding.\n      ")]),e._v(" "),t("div",{attrs:{name:"ProgressiveJPEG","data-unique":"ProgressiveJPEG"}}),e._v(" "),t("h3",[e._v("Progressive JPEG")]),e._v(" "),t("p",[e._v("\n        Progressive JPEG is a lossy mode and is very\n        much similar to the baseline JPEG except that\n        multiple scans are done for coding. It is of two kinds – spectra\n        selection and successive approximation. Some applications use\n        image/pjpeg MIME type for images compressed\n        with this mode. It is suitable for download in slower internet\n        connection where portions of the images can be\n        gradually downloaded.\n      ")]),e._v(" "),t("div",{attrs:{name:"HierarchialJPEG","data-unique":"HierarchialJPEG"}}),e._v(" "),t("h3",[e._v("Hierarchial JPEG")]),e._v(" "),t("p",[e._v("\n        This is a lossless mode based on DCT. Multiple\n        frames of differential and non-differential kinds are used. This mode\n        provides multiple resolutions and the images compressed with this mode\n        take more space.\n      ")]),e._v(" "),t("div",{attrs:{name:"ApplicationsofJPEGformat","data-unique":"ApplicationsofJPEGformat"}}),e._v(" "),t("h3",[e._v("Applications of JPEG format")]),e._v(" "),t("p",[e._v("\n        Digital photography is primarily used for sharing information and\n        storage. JPEG images are widely used for\n        sharing images in internet. Besides just photographs, thumbnails for\n        pages, videos, and other different contents are encoded as\n        JPEG. JPEG is\n        suitable for all kinds of devices – powerful computers as well as mobile\n        devices of low end. It is also appropriate for sharing in all kinds of\n        connections. The capability of level wise compression with minimum loss\n        in the quality of JPEG compression standard\n        makes it very popular and applicable. JPEG is\n        preferred as widely used image compression standard because it is\n        capable of storing 24 bit per pixel color data instead of 8 bit per\n        pixel color data which were used before.\n      ")]),e._v(" "),t("p",[e._v("\n        JPEG is the format used for saving images in\n        digital cameras, storing them, as images in web-pages and everywhere\n        else. The image in ID3 tag of\n        MP3 files for album covers and artist pictures\n        are also encoded as JPEG images.\n      ")]),e._v(" "),t("p",[e._v("\n        Another use of digital photographs is for scientific research and\n        medicine. Scientists and doctors can share their findings, results and\n        problems between each other for better consequences. These can be stored\n        for future also so that future generations of scientists and doctors can\n        study them. In such cases the size of the image wont matter as such but\n        the quality has to be very good. They can zoom in and magnify the minute\n        details which would have been impossible to detect by the normal human\n        eye. Such super high quality of images is supported by\n        JPEG.\n      ")]),e._v(" "),t("p",[e._v("\n        Capturing the personal moments in the form of still photographs is very\n        important as well. In this age of web 2.0 where idea of sharing pictures\n        of what you are doing now to your important friends and families to get\n        connected with each other and remember the moments of personal, cultural\n        and social events is very important as well.\n        JPEG is the best choice for this purpose.\n      ")]),e._v(" "),t("p",[e._v("\n        Digital imagery are also used for surveillance and better representation\n        of raw data in the form of information. Maps make our day to day life\n        easier. These are only the few examples of implementation of Digital\n        Photography and JPEG compression standard to\n        be specific.\n      ")]),e._v(" "),t("div",{attrs:{name:"JPEG’sLimitations","data-unique":"JPEG’sLimitations"}}),e._v(" "),t("h3",[e._v("JPEG’s Limitations")]),e._v(" "),t("p",[e._v("\n        JPEG has some downsides too. It is a lossy\n        compression standard. It may not be appropriate in cases where the\n        minute details of the image and high quality precision matters more than\n        the storage size and transmission bandwidth.\n        JPEG can easily handle compression ration of\n        10:1 without drastically degrading the quality but this is also\n        disadvantage of the JPEG as it takes longer\n        time to decode or decompress. There always has to be a compromise\n        between time and space. There is already a new and improved standards of\n        JPEG family called\n        JPEG2000 but the transition to this new format\n        is becoming very difficult as very low number of device so only very few\n        improvements have been made in past two decades and there is no sign of\n        that transition happening very soon.\n      ")]),e._v(" "),t("p",[e._v("\n        JPEG is superior when we need rich colored and\n        realistic photograph since it supports 24 bit per pixels but when the\n        quantity of colors of the photograph is lower, it creates overhead and\n        cannot compress as good as other image compression standard like GIFs.\n        Another limitation of JPEG file format is that\n        it cannot handle transparency and very sharp edges.\n      ")]),e._v(" "),t("div",{attrs:{name:"FutureTechnologiesofJPEG","data-unique":"FutureTechnologiesofJPEG"}}),e._v(" "),t("h3",[e._v("Future Technologies of JPEG")]),e._v(" "),t("p",[e._v("\n        Joint Photographic Expert Group (JPEG) in 1992\n        introduced a very popular image compression algorithm and standard\n        called JPEG. It is true that\n        JPEG has been one of the most reliable and\n        easy to use photo standard and thus it is the most popular also but very\n        little improvements have been made by the group since last two decades.\n        Technology needs constant revisions and improvements but\n        JPEG committee instead of improving the\n        existing standard introduced a new photo compression and coding\n        algorithm that is meant to replace JPEG. This\n        standard is called JPEG2000, which was\n        introduced initially in the year 2000.\n        JPEG2000 itself is based on wavelet method and\n        there are two standard file format ‘.jp2′ and ‘.jpx’ . Although its been\n        a decade since JPEG2000 was first introduced\n        people have not accepted the file format. It is also thought that\n        JPEG2000 is only a modest improvement to\n        existing JPEG compression standard so until\n        there is something very revolutionary there is no need to totally shift\n        to new technology.\n      ")]),e._v(" "),t("p",[e._v("\n        Now JPEG2000 is the improved standard of photo\n        compression from Joint Photographic Expert Group committee itself but\n        there are lots of other versions of JPEG that\n        have been standardized and are in the development process. Examples of\n        such is JPEG\n        XR where XR stands\n        for eXtended Range and JPEG Mini.\n        JPEG XR is also\n        compression standard for still photographs and digital graphics which is\n        based on technology which is developed by Microsoft under\n        HD Photo standard. Although\n        JPEG XR is a\n        standard that improved the deficits of\n        JPEG but then JPEG\n        XR is not a open standard and patented by\n        Microsoft and so there are very few vendors who support this standard\n        like Microsoft itself and Adobe. There are bunch of other so called\n        improvements to JPEG that are either waiting\n        to be standardized internationally or adopted by peoples. The\n        JPEG mini which is also developed by\n        independent Israeli group is compatible with existing\n        JPEG and compresses the images 5-10 times more\n        without losing the original quality. It can be said that this is in very\n        infant stage and still need some improvements.\n      ")]),e._v(" "),t("div",{attrs:{name:"MPEG","data-unique":"MPEG"}}),e._v(" "),t("h2",[e._v("MPEG")]),e._v(" "),t("div",{attrs:{name:"Introduction39","data-unique":"Introduction39"}}),e._v(" "),t("h3",[e._v("Introduction")]),e._v(" "),t("p",[e._v("\n        MPEG, pronounced ‘M-Peg’, stands for ‘Moving\n        Picture Experts Group’. Like JPEG,\n        MPEG can be used to refer to the group itself\n        or the standards the group has created for coding audio-visual data.\n        There are various digital video encoding formats or codecs, among which\n        some popular ones are 3GPP (popular on mobile\n        phones), AVI (Microsoft’s Audio Video\n        container codec derived from RIFF),\n        FLV (Flash Video, popular on web pages,\n        utilizes Adobe Flash Technology), MOV (Apple’s\n        QuickTime multimedia format), etc. While these formats have their own\n        special capabilities and abundance in particular platforms, codecs from\n        the MPEG family are the most widespread and\n        accepted ones. The official homepage of\n        MPEG is http://mpeg.chiariglione.org/ whereas\n        resources and references are available on http://www.mpeg.org.\n      ")]),e._v(" "),t("p",[e._v("\n        MPEG files are generally represented with the\n        extension ‘.mpg’. Its MIME type\n        is ‘video/mpeg’.\n      ")]),e._v(" "),t("div",{attrs:{name:"HistoryofMPEG","data-unique":"HistoryofMPEG"}}),e._v(" "),t("h3",[e._v("History of MPEG")]),e._v(" "),t("p",[e._v("\n        With an objective to provide better quality videos with small file\n        sizes, ISO, IEC ,\n        SC2 and few other organizations initiated the\n        movement in January 1988. The following table gives an overview of the\n        early phases and the MPEG-1 and\n        MPEG-2 development.\n      ")]),e._v(" "),e._m(39),e._v(" "),t("div",{attrs:{name:"StandardsoftheMPEGFamily","data-unique":"StandardsoftheMPEGFamily"}}),e._v(" "),t("h3",[e._v("Standards of the MPEG Family")]),e._v(" "),t("div",{attrs:{name:"MPEG-1","data-unique":"MPEG-1"}}),e._v(" "),t("h3",[e._v("MPEG-1")]),e._v(" "),e._m(40),e._v(" "),t("p",[e._v("\n        MPEG-1 Layer III is\n        commonly known as MP3.\n      ")]),e._v(" "),e._m(41),e._v(" "),e._m(42),e._v(" "),t("div",{attrs:{name:"MPEG-2","data-unique":"MPEG-2"}}),e._v(" "),t("h3",[e._v("MPEG-2")]),e._v(" "),e._m(43),e._v(" "),t("div",{attrs:{name:"MPEG-3","data-unique":"MPEG-3"}}),e._v(" "),t("h3",[e._v("MPEG-3")]),e._v(" "),t("p",[e._v("\n        MPEG-3 was expected to be the standard for\n        high definition television but it was realized that\n        MPEG-2 was capable of fulfilling the need with\n        extensions and therefore MPEG-3 doesn’t exist\n        as a coding standard.\n      ")]),e._v(" "),t("div",{attrs:{name:"MPEG-4","data-unique":"MPEG-4"}}),e._v(" "),t("h3",[e._v("MPEG-4")]),e._v(" "),t("p",[e._v("\n        In early 1995, development of the next standard was initiated so that\n        low bit rates could also be supported. This standard was called\n        MPEG-4 and it reached\n        CD (Committee Draft) status only in March 1998\n        and the final approval was obtained in the end of the same year. Since\n        then, twenty seven parts of MPEG-4 standard\n        has been published while Part 28 is under development. However,\n        MPEG-4 is not limited to videos with low bit\n        rates, it is becoming the generic standard for video coding.\n      ")]),e._v(" "),t("p",[e._v("\n        MPEG-4 employs object-based compression\n        technique which endorses more efficient and scalable compression with\n        wide range of bit rates. The object-based interaction also enables\n        developers to control each objects of a scene independently and also\n        enable the interactivity among the objects.\n        MPEG-4 Audio or as it was specified as\n        ISO/IEC 14496-3\n        determines the audio coding and composition standards with support for\n        very low to high bitrates. Audio components are also objects which have\n        natural and synthetic quality.\n      ")]),e._v(" "),t("p",[e._v("\n        MJPEG is not to be confused with\n        MPEG. MJPEG nothing\n        but a sequence of image frames that uses\n        JPEG compression for each frame.\n      ")]),e._v(" "),t("div",{attrs:{name:"VideoFundamentals","data-unique":"VideoFundamentals"}}),e._v(" "),t("h3",[e._v("Video Fundamentals")]),e._v(" "),e._m(44),e._v(" "),e._m(45),e._v(" "),e._m(46),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Frame and its fields",src:"/media/mpeg/13.jpg"}}),e._v(" "),t("br"),e._v(" "),t("b",[e._v("Figure 3.2 : Frame and its fields")])]),e._v("\n      A "),t("b",[e._v("progressive video")]),e._v(" or a non-interlaced video has its each pixel\n      refreshed starting from left top, one column by another.\n      NTSC (National Television Systems Committee),\n      PAL (Phase Alternating Line),\n      SECAM (Séquentiel couleur à mémoire) are the\n      most common analog television color encoding systems.\n      PAL videos use\n      YUV color model. Videos of bit-rate 6 Mbit/s is\n      recognized to be best for terrestrial video broadcasting systems\n      considering both the compression and quality factors.\n      "),t("p"),e._v(" "),e._m(47),e._v(" "),e._m(48),e._v(" "),e._m(49),e._v(" "),e._m(50),e._v(" "),t("div",{attrs:{name:"Frames","data-unique":"Frames"}}),e._v(" "),t("h3",[e._v("Frames")]),e._v(" "),e._m(51),e._v(" "),t("p",[e._v("\n        Each one of these frames has different properties and purposes, as\n        explained below.\n      ")]),e._v(" "),t("div",{attrs:{name:"I-frames","data-unique":"I-frames"}}),e._v(" "),t("h3",[e._v("I-frames")]),e._v(" "),t("p",[e._v("\n        I-frame is short for intraframe or intra-coded frame. It is also called\n        keyframe as it is the most important kind of frame. It is independent\n        since this frame doesn’t need other frames for being decoded. It\n        contains more amount of bits or information compared to other kind of\n        frames and therefore is the least compressible, thus taking more storage\n        and bandwidth. The more I-frames a video has, better is\n        its quality.\n      ")]),e._v(" "),t("div",{attrs:{name:"P-frames","data-unique":"P-frames"}}),e._v(" "),t("h3",[e._v("P-frames")]),e._v(" "),t("p",[e._v("\n        P-frame is the short name for predictive frame or predicted frame.\n        P-frames depend on preceding I-frame or P-frame for information like\n        content and color changes, thus the name predictive. It is more\n        compressible than I-frames.\n      ")]),e._v(" "),t("div",{attrs:{name:"B-frames","data-unique":"B-frames"}}),e._v(" "),t("h3",[e._v("B-frames")]),e._v(" "),t("p",[e._v("\n        B-frames means bidirectionally-predictive-coded frames frames or simply\n        bi-directional frames. Bi-directional frames do not include information\n        contained in preceding or following I-frames or P-frames and thus depend\n        upon previous and next frame during decoding and decompression and thus\n        the name bidirectional. It provides highest level of compression\n        compared to the other two kinds of frames. B-frames are never\n        reference frames.\n      ")]),e._v(" "),e._m(52),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"An example frame sequence in videos",src:"/media/mpeg/sequence.jpg"}}),e._v(" "),t("b",[e._v("Figure 3.3: An example frame sequence in videos")])]),e._v(" "),t("b",[e._v("GOP (Group of Pictures)")]),t("br"),e._v("\n      A Group of Pictures is a collection of frames. A\n      GOP (contains at least one I-frame. In an\n      averagely compressed NTSC video, usually every\n      15 th or so frames are I-frames although the\n      MPEG standard isn’t strict about this. A\n      GOP may look like\n      IBBPBBPBBPBB, where I stands for I-frames, B for\n      B-frames and P for P-frames. The arrangement of frames determines the\n      GOP structure which is ultimately one of the\n      factors for determining the compression ratio.\n      "),t("p"),e._v(" "),e._m(53),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"Open and Closed GOPs",src:"/media/mpeg/gop.png"}}),e._v(" "),t("b",[e._v("Figure 3.4: Comparison of Open and Closed GOPs")])]),e._v(" "),t("p"),e._v(" "),t("div",{attrs:{name:"Macroblock","data-unique":"Macroblock"}}),e._v(" "),t("h3",[e._v("Macroblock")]),e._v(" "),e._m(54),e._v(" "),t("div",{attrs:{name:"Compression","data-unique":"Compression"}}),e._v(" "),t("h3",[e._v("Compression")]),e._v(" "),t("div",{attrs:{name:"PsychovisualCompression53","data-unique":"PsychovisualCompression53"}}),e._v(" "),t("h3",[e._v("Psychovisual Compression")]),e._v(" "),t("p",[e._v("\n        Psychovisual compression on MPEG is done\n        similarly as in JPEG with\n        Chroma subsampling.\n      ")]),e._v(" "),t("p",[e._v("\n        MPEG usually utilizes the 4:2:0 sub-sample\n        configuration in most of its implementations like DVDs, etc. Also,\n        MJPEG’s common implementation includes 4:2:0\n        configuration. MPEG also utilizes 4:1:1\n        subsample pattern. NTSC videos in\n        DVCPRO, DV and\n        DVCAM use this pattern. The 4:4:4 subsampling\n        can found in MPEG-4 Part 2 and\n        MPEG-4 since it is considered to be a high\n        quality sampling scheme.\n      ")]),e._v(" "),t("div",{attrs:{name:"SpatialCompression","data-unique":"SpatialCompression"}}),e._v(" "),t("h3",[e._v("Spatial Compression")]),e._v(" "),t("p",[e._v("\n        I-frames being independent frame are simply just still pictures. Spatial\n        redundancy removal is an intra-picture compression technique. Spatial\n        meaning relating to space, this technique is concerned with removing\n        redundancy over the space of two dimensional plane and not the time\n        dimension. It takes advantage of the fact that a pixel value can be\n        predicted from its neighboring pixels to some extent. Also, human eye is\n        not capable of detecting small changes in the image. This removal\n        technique is also known as backward prediction. This prediction is done\n        within an I-frame and does not depend upon information in preceding or\n        following frames. This kind of prediction is also known as\n        backward prediction.\n      ")]),e._v(" "),t("div",{attrs:{name:"Compression/EncodingofI-frames","data-unique":"Compression/EncodingofI-frames"}}),e._v(" "),t("h4",[e._v("Compression/Encoding of I-frames")]),e._v(" "),t("p",[e._v("\n        An I-frame is chambered into different macroblocks , say of 8 by 8\n        blocks of pixels. The macroblocks are then transformed using Discrete\n        Cosine Transform which results in a 8×8 matrix of coefficients. This\n        transformation is not lossy which means the inverse of cosine\n        transformation gives us the original block.\n        DCT-II, sometimes\n        also DCT-I is used for compression of\n        correlated pixels into frequency variations. The two dimensional\n        block-based DCT of 8×8 matrix is used for\n        encoding blocks of video. This encoding standard is defined by\n        IEEE 1180 to increase accuracy and reduce\n        mismatch errors.\n      ")]),e._v(" "),e._m(55),e._v(" "),e._m(56),e._v(" "),t("div",{attrs:{name:"Decoding/DecompressionofI-frames","data-unique":"Decoding/DecompressionofI-frames"}}),e._v(" "),t("h4",[e._v("Decoding/Decompression of I-frames")]),e._v(" "),t("p",[e._v("\n        The I-frames in MPEG video are required to be\n        decoded in order to be played by the media player or the video broadcast\n        receiver (e.g. television). The decoding process is more or less the\n        inverse of the encoding process. The decoding process starts from the\n        Huffman Decoding. Then Run Length Decoding is performed and the result\n        is fed for Inverse Quantization. Quantization process being a lossy one,\n        the matrix after inverse quantization during encoding is not exactly the\n        same as the matrix before quantization during the encoding process. The\n        inversely quantized matrix is then passed through Inverse Discrete\n        Cosine Transform (IDCT) to obtain the\n        macroblocks. The macroblocks are structured to obtain the I-frame which\n        is very much similar to the I-frame where the compression started.\n      ")]),e._v(" "),t("div",{attrs:{name:"TemporalPrediction","data-unique":"TemporalPrediction"}}),e._v(" "),t("h3",[e._v("Temporal Prediction")]),e._v(" "),t("p",[e._v("\n        Temporal prediction takes advantage of the fact that neighbor frames in\n        a video have similar information. Temporal meaning relating to time,\n        temporal redundancy removal is concerned with representing repeating\n        information in consecutive or close frames only once by employing\n        different algorithms. Temporal prediction involves coding of P-frames\n        and B-frames.\n      ")]),e._v(" "),t("p",[e._v("\n        Temporal compression provides more compression than spatial compression\n        as seen on I-frames, because very small information is required to\n        specify the minor changes between the frames. Objects may differ with\n        shift in position, rotation or intensity and the new object can be\n        created just by knowing how different is it from the original object.\n        Storing the original and changed object takes a lot more space than\n        storing the original object and and just the difference.\n      ")]),e._v(" "),t("div",{attrs:{name:"EncodingofP-framesandB-frames","data-unique":"EncodingofP-framesandB-frames"}}),e._v(" "),t("h4",[e._v("Encoding of P-frames and B-frames")]),e._v(" "),e._m(57),e._v(" "),e._m(58),e._v(" "),t("p",[e._v("\n        Instead of basing the blocks on integer values, half-pels can also be\n        taken as the basic unit which may increase the compression ratio but\n        this eats a lot more computational resources. Several other methods\n        exist for motion estimation and compensation like Pel-Recursive\n        Algorithm (PRA), phase correlations, Bayesian\n        estimations, etc. But block-based compensation is believed to be the\n        most efficient one.\n      ")]),e._v(" "),e._m(59),e._v(" "),t("div",{attrs:{name:"DecodingofDeltaFrames","data-unique":"DecodingofDeltaFrames"}}),e._v(" "),t("h4",[e._v("Decoding of Delta Frames")]),e._v(" "),t("p",[e._v("\n        Encoding of delta frames, i.e. P-frames and B-frames involves\n        computation of delta factors which are the displacements of macroblocks\n        and the residual values. During the process of decoding these factors\n        are applied on the reference frame to obtain the candidate frame which\n        can then be displayed. Since the residual value goes compression during\n        encoding it has to be decoded using entropy decoding, inverse\n        quantization and IDCT. This decompressed\n        coefficient of residual and the motion vector gives the prediction error\n        or difference between reference and candidate frame. So, adding this\n        difference to the reference frame gives us the required candidate frame\n        which is not exactly the same frame before encoding due to lossy\n        algorithms used but is of considerably good quality.\n      ")]),e._v(" "),t("p",[e._v("\n        MPEG videos are not suitable for editing\n        because of the presence of the reference frames since P-frames and\n        B-frames depend upon I-frames and other P-frames for being decoded.\n        Cutting a frame could break other frames which depend upon it. Formats\n        specially created for editing videos keep all frames as independent\n        I-frames. The interdependence of frames is also the reason why all\n        MPEG decoders are forced to have encoders\n        built in within themselves.\n      ")]),e._v(" "),t("div",{attrs:{name:"CompressionofAudio","data-unique":"CompressionofAudio"}}),e._v(" "),t("h3",[e._v("Compression of Audio")]),e._v(" "),t("p",[e._v("\n        Audio in MPEG is compressed with techniques\n        like sub-band filtering, psychoacoustic model and quantization of\n        digital audio. Audio signals are divided into frequency sub-bands using\n        convolution filters. There are 32 critical sub-bands available.\n        Psychoacoustics utilizes the aspects and processes like limits of\n        perception, masking effects and sound localization.\n      ")]),e._v(" "),t("div",{attrs:{name:"QuantizationofDigitalAudio","data-unique":"QuantizationofDigitalAudio"}}),e._v(" "),t("h4",[e._v("Quantization of Digital Audio")]),e._v(" "),t("p",[e._v("\n        Quantization of digital audio means converting of sound waves to a\n        distribution of individual samples each having unique magnitude of\n        amplitude. Bit depth is what is used to define the range of levels of\n        amplitude. An 8-bit quantization means there are total of 256 possible\n        values for the amplitude levels whereas 16-bit quantization means there\n        can be total of 65,536 possible values of amplitude. Autocorrecting\n        music tracks is also done with quantization in which the beats are\n        distributed evenly to remove errors on timing by analyzing and\n        stretching in time.\n      ")]),e._v(" "),t("p",[e._v("\n        MDCT (Modified Discrete Cosine Transform) is a\n        variant of DCT-IV in\n        which the various transforms are overlapped.\n        MDCT is used in audio compression in the\n        MPEG-1 Audio Layer 3 and\n        MPEG-2 Audio Layer 3, i.e.\n        MP3 as well as in\n        AAC and Vorbis encoding.\n      ")]),e._v(" "),t("div",{attrs:{name:"VBRandCBR","data-unique":"VBRandCBR"}}),e._v(" "),t("h4",[e._v("VBR and CBR")]),e._v(" "),t("p",[e._v("\n        With VBR (Variable Bit Rate), higher bit rate\n        may be assigned to the segments with higher complexity of audio or video\n        file and lower for less complex segments. The bitrate for the whole file\n        is represented as the average bitrate. In contrast, in\n        CBR (Constant Bit Rate), all segments are\n        given the same bit-rate. VBR provides more\n        flexibility, accuracy and quality whereas\n        CBR provides more compatibility with devices,\n        software and connections.\n      ")]),e._v(" "),t("div",{attrs:{name:"ImplementationsofMPEG","data-unique":"ImplementationsofMPEG"}}),e._v(" "),t("h3",[e._v("Implementations of MPEG")]),e._v(" "),t("div",{attrs:{name:"Codecs","data-unique":"Codecs"}}),e._v(" "),t("h3",[e._v("Codecs")]),e._v(" "),t("p",[e._v("\n        Most televisions, multimedia players have support for decoding\n        MPEG. Several codecs are available for\n        different operating systems. A codec (compressor-decompressor) is a\n        software or hardware that can encode/compress and/or decode/decompress\n        digital stream of data. A software codec may not be a stand-alone\n        program but just a library or module. Xvid is an example of codec which\n        is the open-source codec that implements the\n        MPEG-4 standard. FFmpeg is one of the most\n        popular projects for producing free audio/video codec libraries for\n        MPEG. Interestingly, FFmpeg uses this zigzag\n        pattern used for entropy encoding of\n        MPEG videos in their logo.\n      ")]),e._v(" "),t("p"),e._v(" "),t("center",[t("br"),e._v(" "),t("img",{attrs:{alt:"FFmpeg Logo showing zigzag pattern",src:"/media/mpeg/ffmpeg.jpg"}}),e._v(" "),t("b",[e._v("Figure 3.5: FFmpeg Logo showing zigzag pattern")])]),e._v(" "),t("p"),e._v(" "),t("div",{attrs:{name:"MP3","data-unique":"MP3"}}),e._v(" "),t("h3",[e._v("MP3")]),e._v(" "),t("p",[e._v("\n        MP3 is a popular name for\n        MPEG-1 and MPEG-2\n        Audio Layer III. It is the most widely used\n        audio encoding format. MP3 standard was\n        finalized in 1992 and released in 1993. MP3 is\n        a lossy format. It supports both CBR (Constant\n        Bit Rate) and VBR (Variable Bit Rate)\n        encoding. MP3 files usually have ‘.mp3′ file\n        extension and their standard MIME type is\n        ‘audio/mpeg’. MP3 has support for\n        ID3 and other kinds of tags for storing\n        metadata and DRM information of the audio\n        file. The MP3 encoding format is patented.\n        MP3 is the most popular audio file format and\n        is used for music tracks, online streaming, audio recording, etc.\n      ")]),e._v(" "),t("div",{attrs:{name:"MP4","data-unique":"MP4"}}),e._v(" "),t("h3",[e._v("MP4")]),e._v(" "),t("p",[e._v("\n        MP4 is a common name for\n        MPEG-4 Part 14.\n        MP4 is a container for digital video and audio\n        streams. MP4 files can also contain subtitles,\n        images and hint tracks. The hint track is what makes\n        MP4 easily possible to be streamed over the\n        internet. MP4 encoding is specified in the\n        standard ISO/IEC\n        14496-14. It is heavily influenced from the QuickTime File Format.\n        MP4 files are represented with the file\n        extension ‘.mp4′.\n      ")]),e._v(" "),t("div",{attrs:{name:"ApplicationsofJPEG","data-unique":"ApplicationsofJPEG"}}),e._v(" "),t("h3",[e._v("Applications of JPEG")]),e._v(" "),t("p",[e._v("\n        MPEG, being the most popular video coding\n        standard, has very wide range of applications.\n      ")]),e._v(" "),t("div",{attrs:{name:"TelevisionandBroadcasting","data-unique":"TelevisionandBroadcasting"}}),e._v(" "),t("h3",[e._v("Television and Broadcasting")]),e._v(" "),t("p",[e._v("\n        All terrestrial, cable or broadcasting television technologies like\n        DBS (Direct Broadcast Satellite),\n        DVB (Digital Video Broadcasting),\n        ISDB-T, HDTV (High\n        Definition Television), CATV (Cable\n        Televisions) depend on MPEG. Most terrestrial\n        television systems use MPEG-2 although some\n        use MPEG-1 for certain purposes.\n        ATSC (Advanced Television Systems Committee)\n        have also standardized MPEG-2 as the official\n        encoding format. Smart Television systems like Apple\n        TV also support\n        MPEG-4 video up to 2.5 Mbps.\n      ")]),e._v(" "),t("div",{attrs:{name:"Internet,Mobile,MultimediaandGaming","data-unique":"Internet,Mobile,MultimediaandGaming"}}),e._v(" "),t("h3",[e._v("Internet, Mobile, Multimedia and Gaming")]),e._v(" "),t("p",[e._v("\n        MPEG is also very popular with online\n        streaming. Since bandwidth is an important factor in internet,\n        MPEG format is usually chosen for its better\n        compression. It is also gaining more popularity in mobile multimedia\n        because of its smaller size. Motion pictures in most video games are\n        also rendered using MPEG.\n      ")]),e._v(" "),t("div",{attrs:{name:"RecordingandCommunication","data-unique":"RecordingandCommunication"}}),e._v(" "),t("h3",[e._v("Recording and Communication")]),e._v(" "),t("p",[e._v("\n        Varieties of digital camcorders and video recorders use\n        MPEG as the default format for recording\n        videos. Products like XDCAM implement in the\n        standard in their own different ways. Different communication activities\n        like video conferencing and video calling also utilize\n        MPEG.\n      ")]),e._v(" "),t("div",{attrs:{name:"StorageandDistribution","data-unique":"StorageandDistribution"}}),e._v(" "),t("h3",[e._v("Storage and Distribution")]),e._v(" "),e._m(60),e._v(" "),t("div",{attrs:{name:"LimitationsofMPEG","data-unique":"LimitationsofMPEG"}}),e._v(" "),t("h3",[e._v("Limitations of MPEG")]),e._v(" "),e._m(61),e._v(" "),t("div",{attrs:{name:"FutureofMPEG","data-unique":"FutureofMPEG"}}),e._v(" "),t("h3",[e._v("Future of MPEG")]),e._v(" "),t("p",[e._v("\n        Huge improvements in video encoding have been made since the\n        introduction of MPEG-1. Every new\n        specification released gives better compression of data and allows more\n        features for video technology. The latest widespread standard\n        MPEG-4 provides lot more flexibility with\n        support for internal subtitles and hint tracks. Online streaming and\n        other certain needs of the modern days may be fulfilled by it is not the\n        most optimal format. It requires lot more improvement to win back the\n        video streaming in internet from other encoding formats like\n        FLV. With booming of Internet and Smart\n        TV systems like Google\n        TV and Apple\n        TV which promise to provide a lot more\n        flexibility for the users, a better standard is required as the\n        currently standardized specifications of the\n        MPEG family may not be enough to support this\n        flexibility. Also, with portable devices gaining more popularity, the\n        standard has to be portable with easy support for wide array of devices.\n        MP4 may be replacing mobile video coding\n        technologies like 3GP but it has a lot more\n        space for improvement. Use of MPEG is not\n        appropriate for all gaming and interactive contents. More profiles have\n        to be added to the standard. Also, there is a room for better\n        compression in audio.\n      ")]),e._v(" "),t("p",[e._v("\n        MPEG-4 Part 28 which is under development has\n        lots of improvements to bring. MPEG\n        DASH led by employees of Microsoft is also\n        believed to overcome many limitation of\n        MPEG-4, as a common adaptive-optimized\n        encoding format. Despite being one of the most popular video encoding\n        standard, MPEG falls short in may applications\n        and its future is uncertain unless huge improvement and flexibility have\n        been added to it.\n      ")]),e._v(" "),e._m(62),e._v(" "),e._m(63),e._v(" "),e._m(64),e._v(" "),e._m(65),e._v(" "),e._m(66),e._v(" "),e._m(67),e._v(" "),e._m(68),e._v(" "),e._m(69),e._v(" "),e._m(70),e._v(" "),e._m(71),e._v(" "),e._m(72),e._v(" "),e._m(73),e._v(" "),e._m(74),e._v(" "),e._m(75),e._v(" "),e._m(76),e._v(" "),e._m(77),e._v(" "),e._m(78),e._v(" "),e._m(79),e._v(" "),e._m(80),e._v(" "),e._m(81),e._v(" "),e._m(82),e._v(" "),e._m(83)],1)],1)}),[function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        This document titled\n        "),t("b",[e._v("MPEG and\n          JPEG Compression")]),e._v("\n        was my college assignment. Although I got an ‘A’ for it, I’m pretty sure\n        there are many errors in this document. References, sources and\n        acknowledgements are at the bottom. Feel free to comment on anything\n        I’ve missed. Here goes!\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The term "),t("b",[e._v("data compression")]),e._v(" refers to the process of reducing the\n        amount of data required to represent a given quantity of information, by\n        using different transformation and/or encoding techniques. Compressed\n        data takes less space for storage and is conveyed faster. Compression\n        sometimes also provides other advantages like security and privacy since\n        analyzing an encoded file is more difficult than analyzing a raw file.\n        Compression generally involves two techniques. The first technique is to\n        throw away the redundant information by representing single sample of\n        data only once and the second technique is to throw away things that\n        have very minimal effect on perception of the end user.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        There are different forms of DCT, which are\n        listed below. Here x,"),t("sub",[e._v("0")]),e._v(", …, x"),t("sub",[e._v("N-1")]),e._v(" are the spatial\n        coordinates whereas X"),t("sub",[e._v("0")]),e._v(" , …, X"),t("sub",[e._v("N-1")]),e._v(" are the frequency\n        coordinates. Both are the sequences of real numbers.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DCT-I of N is given by"),t("br"),e._v(" "),t("img",{attrs:{alt:"DCT-I",src:"/media/mpeg/dct1.png"}}),e._v("\n        where N is a real number greater than or equal to 2,"),t("br"),e._v("\n        x"),t("sub",[e._v("n")]),e._v(" are real and even numbers around n=0 and n=N-1,"),t("br"),e._v("\n        k = 0,1,2,…,N-1 and"),t("br"),e._v("\n        X"),t("sub",[e._v("k")]),e._v(" are real and even numbers around k=0 and k=N-1.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DCT-II is given\n        by"),t("br"),e._v(" "),t("img",{attrs:{alt:"DCT-II Formula",src:"/media/mpeg/dct2.png"}}),e._v("\n        where N is a positive real number ( can be less than 2, unlike in\n        DCT-I)"),t("br"),e._v("\n        x"),t("sub",[e._v("n")]),e._v(" are real and even numbers around n=-1/2 and n=N-1/2,"),t("br"),e._v("\n        k = 0,1,2,…,N-1 and"),t("br"),e._v("\n        X"),t("sub",[e._v("k")]),e._v(" are real and even numbers around k=0 and k=N.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        An example transformation is demonstrated on\n        "),t("a",{attrs:{href:"#figure_transform"}},[e._v("this figure.")])])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DCT-III is given\n        by"),t("br"),e._v(" "),t("img",{attrs:{alt:"DCT-III Formula",src:"/media/mpeg/dct3.png"}}),e._v("\n        where N is a real number,"),t("br"),e._v("\n        x"),t("sub",[e._v("n")]),e._v(" are real and even numbers around n=0 and n=N,"),t("br"),e._v("\n        k = 0,1,2,…,N-1 and"),t("br"),e._v("\n        X"),t("sub",[e._v("k")]),e._v(" are real and even numbers around k=-1/2 and k=N-1/2.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DCT-III can be\n        scaled to get the inverse of DCT-"),t("span",{staticClass:"caps"},[e._v("II")]),e._v("\n        and therefore is also known as Inverse Direct Cosine Transform ("),t("span",{staticClass:"caps"},[e._v("IDCT")]),e._v("), since DCT simply refers to\n        DCT-II.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DCT-IV is given\n        by"),t("br"),e._v(" "),t("img",{attrs:{alt:"DCT-IV Formula",src:"/media/mpeg/dct4.png"}}),e._v("\n        where N is a real number,"),t("br"),e._v("\n        x"),t("sub",[e._v("n")]),e._v(" are real and even numbers around n=-1/2 and odd numbers\n        around n=N-1/2,"),t("br"),e._v("\n        k = 0,1,2,…,N-1 and"),t("br"),e._v("\n        X"),t("sub",[e._v("k")]),e._v(" are real and even numbers around n=-1/2 and odd numbers\n        around n=N-1/2.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Scalar quantization is a one-dimensional quantization process. It treats\n        each value from the input set separately."),t("br"),e._v("\n        If the input value be x and output value be y, scalar quantization is\n        the simple process denoted as:"),t("br"),e._v("\n        y=Q(x)"),t("br"),e._v("\n        Simple example of scalar quantization is constraining a set of real\n        numbers into integer values by rounding each real x value to their\n        closest integer value y. This process is also used in converting analog\n        wave forms to digital samples.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Quantization of a digital image is the technique of judging which\n        sections of the image can be ignored in such a way that the image\n        doesn’t look significantly different. This is a lossy process. The color\n        spectrum is also quantized by reducing the number of colors used to\n        represent it in processes like conversion of\n        JPEG to GIF since\n        GIF only supports 256 colors. Quantization is\n        also done when images are printed because the printers don’t have the\n        tonal resolution which supports all the colors for all pixels in the\n        image. Similar is the case with image scanning specially for shadow\n        areas."),t("br"),e._v("\n        Quantization is a lossy process since it involves approaches like\n        rounding off and discarding negligible entities. The inverse of\n        quantization doesn’t produce exactly the same object which was fed for\n        quantization. Whatever is lost is called quantization noise.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Quantization matrices or quantizers are used for defining the\n        quantization process. Supposing Q[i,j] is the quantizer matrix, every\n        time a matrix of DCT coefficients, say M[i,j],\n        is encountered, it is divided by quantizer matrix Q[i,j] to obtain\n        quantized matrix M"),t("sub",[e._v("q")]),e._v("[i,j]. Here, we are only considering\n        two-dimensional matrices.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The quantization equation can be given as"),t("br"),e._v("\n        M"),t("sub",[e._v("q")]),e._v(" [i,j] = round( M[i,j] / Q[i,j] )\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The inverse quantization equation becomes"),t("br"),e._v("\n        M’[i,j]= M"),t("sub",[e._v("q")]),e._v("[i,j] * Q[i,j]\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The rounding off is not invertible and therefore the process is lossy.\n        The loss is measured as quantization error which is given by:"),t("br"),e._v("\n        Quantization Error = M"),t("sub",[e._v("q")]),e._v("[i,j] – M’"),t("sub",[e._v("q")]),e._v("[i,j]\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        An example:"),t("br"),e._v("\n        Matrix of DCT coefficients, M[i,j]=\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("img",{attrs:{alt:"Matrix of DCT Coefficients",src:"/media/mpeg/matrix-dct.png"}}),e._v(" "),t("br"),e._v("\n        Quantizer Matrix, Q[i,j] ="),t("br"),e._v(" "),t("img",{attrs:{alt:"Quantizer Matrix",src:"/media/mpeg/m2.png"}})])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        M"),t("sub",[e._v("q")]),e._v("[1,1] = round( M[1,1] / Q[1,1] )"),t("br"),e._v("\n        = round ( – 415 / 16)"),t("br"),e._v("\n        = round ( -25.9375)"),t("br"),e._v("\n        = -26\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        and so on, we get"),t("br"),e._v("\n        Quantized Matrix M"),t("sub",[e._v("q")]),e._v("[i,j] ="),t("br"),e._v(" "),t("img",{attrs:{alt:"Quantized Matrix",src:"/media/mpeg/m3-300x185.png"}})])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        VLC is the major part of entropy encoding.\n        VLC is the process of mapping the input\n        symbols into codes of variable lengths. This enables us to compress the\n        the symbols without any error. This means the compression is lossless\n        and decompressing the symbols one by one gives us the same original\n        input. Compression level close to its entropy can be arbitrarily\n        achieved if the right coding strategy is chosen. Also, identically\n        distributed as independent source needs to be selected. Unlike\n        fixed-length coding techniques, VLC can be\n        used for compression of small blocks of data too, with less probability\n        of failure."),t("br"),e._v("\n        Some of the most popular strategies of VLC are\n        Arithmetic Coding, Huffman Coding and Lempel-Ziv coding.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Huffman code is generated using a binary tree. Such binary tree is\n        called Huffman tree."),t("br"),e._v("\n        The process of building is tree is outlined below:\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Step 1 : Independent parentless node is created for each input symbol,\n        including the symbol and the probability of its occurrence."),t("br"),e._v("\n        Step 2: The two parentless nodes with the lowest probabilities are\n        selected."),t("br"),e._v("\n        Step 3: A new parent node is created with the last two selected nodes as\n        immediate children."),t("br"),e._v("\n        Step 4 : The newly created node is assigned the probability equal to the\n        sum of its children."),t("br"),e._v("\n        Step 5 : Continue from Step 2 until only only one parentless node\n        is left.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("h4",[e._v("\n        Context-Adaptive Binary Arithmetic Coding ("),t("span",{staticClass:"caps"},[e._v("CABAC")]),e._v(") and Context-adaptive Variable-length Coding ("),t("span",{staticClass:"caps"},[e._v("CAVLC")]),e._v(")\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        An example of Run Length Encoding:"),t("br"),e._v("\n        Input symbols:\n        AAHHHHHHHTTTTTTPPPPPWWJKKKKLLLLLLLLLL"),t("br"),e._v("\n        Run Length Code : 2A7H6T5P2W1J4K10L\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Another example of RLE that only compresses\n        consecutive zeroes:"),t("br"),e._v("\n        Input Symbols : 0000010000000000010001011000000000000"),t("br"),e._v("\n        Counting the number of zeroes separated by 1’s,"),t("br"),e._v("\n        5 11 3 1 0 12"),t("br"),e._v("\n        In 4-bit code representation, the run length encoding is:"),t("br"),e._v("\n        0101 1011 0011 0001 0000 1100\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        In the early years of television broadcasting, same bandwidth were\n        provided for color components and brightness. Soon it was realized that\n        the human perception is more sensitive to intensity than to colors. So,\n        reduction of the resolution of colors or chroma by keeping the\n        brightness or luma intact didn’t alter the picture quality. And then the\n        television broadcasting companies started saving bandwidth by\n        propagating only half the amount of chroma than luma. This methodology\n        they implemented is called "),t("b",[e._v("chroma subsampling")]),e._v(". This is\n        demonstrated in the figure below :\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Luma")]),e._v(", in imaging and video technology refers to the intensity or\n        brightness of the image. It is sometimes called ‘Luminance’ but by\n        convention, the term ‘Luma’ is to be used in video technology and\n        ‘Luminance’ in general color science. Luminance is the weighted sum of\n        the RGB components of image whereas Luma\n        considers the gamma-compressed components- R’G’B’. If Luminance is\n        represented as Y, Luma is represented as Y’, read as Y prime, where the\n        prime (‘) symbol stands for gamma-compression. The weighted sum is\n        calculated by using coefficients that have recommended. Rec. 709\n        specifies the following expression for computation of luma:\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Chroma")]),e._v(", or sometimes called chrominance, is the signal that\n        carries the color information of the image. It is abbreviated as ‘C’\n        whereas popularly represented as UV, since it\n        is composed of two color-difference components, U and V.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        U= B’ – Y’ (Gamma-compressed Blue – Luma)"),t("br"),e._v("\n        V = R’ – Y’ (Gamma-compressed Red – Luma)\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The Y’UV or\n        YUV color space was used for analog color\n        television systems. For modern digital imaging and video broadcasting\n        the "),t("b",[e._v("YC"),t("sub",[e._v("b")]),e._v("C"),t("sub",[e._v("r")])]),e._v(" family\n        of color spaces are used. C"),t("sub",[e._v("b")]),e._v(" stands for blue chrominance and\n        C"),t("sub",[e._v("r")]),e._v(" stands for red chrominance. "),t("span",{staticClass:"caps"},[e._v("YC")]),t("sub",[e._v("b")]),e._v("C"),t("sub",[e._v("r")]),e._v(" color space is not an absolute or basic color\n        space but is an encoding of the RGB color\n        space. Also, like YUV should not be confused\n        with YC"),t("sub",[e._v("b")]),e._v("C"),t("sub",[e._v("r")]),e._v(" , C"),t("sub",[e._v("b")]),e._v("\n        does not correspond to U and C"),t("sub",[e._v("b")]),e._v(" does not correspond\n        to V.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Almost all digital video formats today use "),t("span",{staticClass:"caps"},[e._v("YC")]),t("sub",[e._v("b")]),e._v("C"),t("sub",[e._v("r")]),e._v(" but they vary by the\n        "),t("b",[e._v("subsampling ratio")]),e._v(". The subsampling ratio is generally expressed\n        in the format "),t("b",[e._v("p:q:r")]),e._v(". Popular explanations use j:a:b or H:V:T\n        symbols for the ratio but the bottom-line of all the explanations is the\n        same. The values ‘p’, ‘q’ and ‘r’ are always integers. The first value\n        ‘p’ gives the horizontal width of the sample region or the number of\n        pixels in the row of consideration. It is usually ‘4’ until recently\n        Sony used the 3:1:1 ratio. 4 is used for p for its traditional\n        references and also because being multiple of 2, dividing it for other\n        factors in the ratio becomes easy. So, it’s generally 4:q:r. The second\n        value ‘q’ gives the count of chroma samples in the first row of ‘p’\n        pixels. So, 4:4:r means there are four chroma samples for a row of four\n        pixels, which implies no subsampling. 4:2:r means there are two chroma\n        samples for a row of four pixels which means the image has been\n        subsampled by the factor of 2. 4:1:r means there is a single chroma\n        sample used for four pixels of the row and thus the image is subsampled\n        by the factor of 4. The third value in the ratio, ‘r’ says only if\n        vertical subsampling has been done. If ‘r’ is equal to ‘q’, vertical\n        subsampling is absent that is same chroma samples haven’t been forced\n        for vertical pixels. If ‘r’ is zero, vertical subsampling has been done\n        for the image.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Since pictures and videos are the basic types of multi-media, we now\n        look into popular"),t("br"),e._v("\n        encoding formats of each kind, JPEG and\n        MPEG.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        JPEG, pronounced ‘Jay-Peg’, stands for ‘Joint\n        Photographers Expert Group’. JPEG is also used\n        to refer to the family of standards this group has created for coding\n        and compression of still images. JPEG standard\n        is the most widespread standard for representation of still images.\n        Images can be represented in other different formats like Bitmap ("),t("span",{staticClass:"caps"},[e._v("BMP")]),e._v("), Tagged Image File Format (TIFF), Portable\n        Network Graphics (PNG), Graphics Interchange\n        Format (GIF), et cetera.\n        TIFF is mostly used with Optical Character\n        Recognition, and GIF is popular with animation\n        (not still images). Lossless PNG files are\n        preferred for editing images whereas JPEG is\n        usually preferred for distribution because of its great compression\n        abilities which makes it more portable and yet with considerable quality\n        despite being lossy.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        JPEG, technically, is a compression method\n        whereas the file format is defined to be\n        JFIF (JPEG File\n        Interchange Format). In general usage,\n        JFIF files are called ‘"),t("span",{staticClass:"caps"},[e._v("JPEG")]),e._v("\n        images’ and are represented with file extensions – .jpeg, .jpg, .jfif,\n        .jfi. The MIME type for\n        JFIF file format as specified in\n        RFC 2046 in 1996 is ‘image/jpeg’. A\n        JPEG file has the bytes ‘"),t("span",{staticClass:"caps"},[e._v("FFD8")]),e._v("′ in its beginning and ‘FF D9′ at its end.\n        ‘4A 46 49 46′, which is the ASCII code for the\n        string ‘JFIF’ is used as a null terminated\n        string in JPEG/"),t("span",{staticClass:"caps"},[e._v("JFIF")]),e._v(" files.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Joint Photographers Expert Group is a joint committee of the\n        standardization bodies ISO/"),t("span",{staticClass:"caps"},[e._v("IEC")]),e._v("\n        JTC 1 and ITU-T\n        (International Telecommunications Union).\n        ISO/IEC\n        JTC 1 in turn is the joint committee of\n        ISO (International Organization for\n        Standardization) and IEC (International\n        Electrotechnical Commission). In fact, JPEG is\n        just a nickname to this joint group. The official homepage of the group\n        is\n        "),t("a",{attrs:{title:"Official homepage of JPEG",href:"http://web.archive.org/web/20170930220816/http://www.jpeg.org/",target:"_blank"}},[e._v("http://www.jpeg.org")]),e._v(". The committee meets at least three times a year and has been\n        publishing parts of the JPEG,\n        JPEG-LS,\n        JPEG 2000, JPSearch and\n        JPEG\n        XR standards.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        JPEG-LS is the\n        standard for lossless or near-losless compression. Part 1 and Part 2 of\n        this standard were released in 1998 and 2002, which specified the\n        baseline and the extensions of the standard, respectively. The group has\n        also published MRC (Mixed Raster Content) in\n        1999 which is used for image segmentation. Six parts of JPSearch\n        standard has been released starting from 2007, of which Part 2, 5 and 6\n        still being under development. JPEG\n        XR standard has five parts published, Part 1\n        surprisingly being the latest one to be revised. Also, the group is\n        developing AIC (Advanced Image Coding) with\n        major involvement from ISO/"),t("span",{staticClass:"caps"},[e._v("IEC")]),e._v(".\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("span",{staticClass:"quo"},[e._v("‘")]),e._v("JPEG 2000′, not to\n        be confused with the JPEG standard, is another\n        image compression standard the group has been publishing since 2000 and\n        its Part 14 is currently under development.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        JPEG makes use of\n        "),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://motorscript.com/mpeg-jpeg-compression/#Chromasubsampling",title:"Chroma Subsampling"}},[e._v("Chroma Subsampling")]),e._v("\n        as a psychovisual compression technique. This technique exploits the\n        fact that human eye is far less sensitive to the variation in hue than\n        that in brightness. JPEG utilizes the 4:2:0\n        and 4:1:1 subsample ratio, 4:2:0 being the usual one.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Block preparation is the first step of the compression. Blocks of 8 x 8\n        pixels are segmented. A block then can be represented in a 8 x 8 matrix.\n        The spatial variation of pixels in each matrix is converted into\n        frequency variations using two dimensional Discrete Cosine Transform."),t("br"),e._v(" "),t("a",{attrs:{name:"figure_transform"}}),t("br")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("table",[t("tbody",[t("tr",[t("th",[e._v("Date")]),e._v(" "),t("th",[e._v("Place (if meeting)")]),e._v(" "),t("th",[e._v("Description")])]),e._v(" "),t("tr",[t("td",[e._v("May 1988")]),e._v(" "),t("td",[e._v("Ottawa, Canada")]),e._v(" "),t("td",[e._v("\n              First official meeting."),t("br"),e._v("\n              Relate motion video to digital storage devices."),t("br"),e._v("\n              Real-time decoding at bitrate of 1.5Mbit/s.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("18-22 July 1988")]),e._v(" "),t("td",[e._v("Politecnico di Torino, Italy")]),e._v(" "),t("td",[e._v("\n              IVC (Internet Video Coding) and\n              CDVS was proposed\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("September 1988")]),e._v(" "),t("td",[e._v("London, UK")]),e._v(" "),t("td",[e._v("Goals and objectives were hammered.")])]),e._v(" "),t("tr",[t("td",[e._v("December 1988")]),e._v(" "),t("td",[e._v("Hannover, Germany")]),e._v(" "),t("td",[e._v("\n              Audio coding activity initiated."),t("br"),e._v("\n              Video testing sequences were selected by video group.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("February 1989")]),e._v(" "),t("td",[e._v("Livingston, New Jersey, USA")]),e._v(" "),t("td",[e._v("Techniques for matrix testing were specified")])]),e._v(" "),t("tr",[t("td",[e._v("June 1, 1989")]),e._v(" "),t("td",[e._v("-")]),e._v(" "),t("td",[e._v("Deadline for audio standard development participation.")])]),e._v(" "),t("tr",[t("td",[e._v("May 1989")]),e._v(" "),t("td",[e._v("Rennes, France")]),e._v(" "),t("td",[e._v("Dedicated meeting for description of proposal packages.")])]),e._v(" "),t("tr",[t("td",[e._v("July 1989")]),e._v(" "),t("td",[e._v("Stockholm, Sweden")]),e._v(" "),t("td",[e._v("\n              Finalization of video package."),t("br"),e._v("\n              Initiation of activities on MPEG system.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("October 1989")]),e._v(" "),t("td",[e._v("Kurihama and OSaka, Japan")]),e._v(" "),t("td",[e._v("\n              Proposals for decompressed digital image sequences were made\n              available.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("January 1990")]),e._v(" "),t("td",[e._v("Eindhoven, Netherlands")]),e._v(" "),t("td",[e._v("\n              Semi-independent testings were made possible by core experiments.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("March 1990")]),e._v(" "),t("td",[e._v("Tampa, Florida, USA")]),e._v(" "),t("td",[e._v("\n              Results from core experiments were analyzed."),t("br"),e._v("\n              JPEG vs. H.261 comparison was run for\n              picture coding."),t("br"),e._v("\n              Syntax in pseudocode was defined."),t("br"),e._v("\n              SM1 (First Simulation Model) for video\n              was bring forth.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("April 1990")]),e._v(" "),t("td",[e._v("Washington, USA")]),e._v(" "),t("td",[e._v("\n              Working Group – WG8 was restructured.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("May 1990")]),e._v(" "),t("td",[e._v("Torino, Italy")]),e._v(" "),t("td",[e._v("\n              MPEG-2 with high definition coding was\n              proposed.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("September 1990")]),e._v(" "),t("td",[e._v("Santa Clara, California, USA")]),e._v(" "),t("td",[e._v("\n              Macroblock level was selected for motion compensation."),t("br"),e._v("\n              WD (Working Draft) was created.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("December 1990")]),e._v(" "),t("td",[e._v("Berlin, Germany")]),e._v(" "),t("td",[e._v("\n              MPEG-1 standard ("),t("span",{staticClass:"caps"},[e._v("WD")]),e._v("\n              11172) was edited."),t("br"),e._v("\n              Requirements for MPEG-2 were specified.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("March 1991")]),e._v(" "),t("td",[e._v("San Jose, California, USA")]),e._v(" "),t("td",[e._v("Meetings canceled due to Gulf War crisis.")])]),e._v(" "),t("tr",[t("td",[e._v("May 1991")]),e._v(" "),t("td",[e._v("Paris france")]),e._v(" "),t("td",[e._v("\n              WD 11172 was edited and cleaned up.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("August 1991")]),e._v(" "),t("td",[e._v("Santa Clara, California, USA")]),e._v(" "),t("td",[e._v("\n              MPEG-1 WD was\n              prepared to promote to CD (Committee\n              Draft).\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("November 1991")]),e._v(" "),t("td",[e._v("Kurihama, Japan")]),e._v(" "),t("td",[e._v("\n              MPEG-1 WD was\n              accepted as CD.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("January 1992")]),e._v(" "),t("td",[e._v("Haifa, Israel")]),e._v(" "),t("td",[e._v("\n              WG11 created\n              DIS (Draft International Standard)\n              11172.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("July 1992")]),e._v(" "),t("td",[e._v("Angra dos Reis, Brazil")]),e._v(" "),t("td",[e._v("\n              TM1 (Test Model 1) for\n              MPEG-2 was made available.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("September 1992")]),e._v(" "),t("td",[e._v("Tarrytown, New York, USA")]),e._v(" "),t("td",[e._v("\n              Discussions with cable companies for likely application of\n              MPEG-2 for cable television.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("November 1992")]),e._v(" "),t("td",[e._v("London, UK")]),e._v(" "),t("td",[e._v("\n              First WD for\n              MPEG-2 was created."),t("br"),e._v("\n              It was realized that MPEG-2 could\n              fulfill what MPEG-3 was expected to.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("January 1993")]),e._v(" "),t("td",[e._v("Rome, Italy")]),e._v(" "),t("td",[e._v("\n              Technical details for MPEG-2 were worked\n              out.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("April 1993")]),e._v(" "),t("td",[e._v("Sydney, Australia")]),e._v(" "),t("td",[e._v("\n              Main profile of MPEG-2 was frozen."),t("br"),e._v("\n              Macroblock stuffing was discarded.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("July 1993")]),e._v(" "),t("td",[e._v("New York, USA")]),e._v(" "),t("td",[e._v("\n              Started to clean the scalabilty aspects of\n              WD.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("September 1993")]),e._v(" "),t("td",[e._v("Brussels, Belgium")]),e._v(" "),t("td",[e._v("Cleaning up of scalabilty continued.")])]),e._v(" "),t("tr",[t("td",[e._v("November 1993")]),e._v(" "),t("td",[e._v("Seoul, Korea")]),e._v(" "),t("td",[e._v("\n              MPEG-2\n              CD 13818 was published.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("March 1994")]),e._v(" "),t("td",[e._v("Paris, France")]),e._v(" "),t("td",[e._v("DIS 13818 was prepared.")])]),e._v(" "),t("tr",[t("td",[e._v("July 1994")]),e._v(" "),t("td",[e._v("Norway")]),e._v(" "),t("td",[e._v("\n              MPEG-2 Part 6 ("),t("span",{staticClass:"caps"},[e._v("DSM")]),e._v("\n              CC) was published.\n            ")])]),e._v(" "),t("tr",[t("td",[e._v("November 1994")]),e._v(" "),t("td",[e._v("Singapore")]),e._v(" "),t("td",[e._v("\n              MPEG-1 and\n              MPEG-2 achieved final approval.\n            ")])])])])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG-1 was designed for encoding multimedia\n        with videos of VHS (Video Home System) quality\n        and audio of bit rate up to 1.5 Mbit/s. MPEG-1\n        standard, specified as ISO/"),t("span",{staticClass:"caps"},[e._v("IEC")]),e._v("\n        11172, has 5 parts, viz. Systems, Video, Audio, Compliance testing and\n        Software simulation. This standard is highly influenced by the standards\n        JPEG and H.261 developed by\n        ITU-T. Resolutions up to 4095×4095 and bitrate\n        up to 100 Mbit/s is supported by MPEG-1. 4:2:0\n        color space is only supported. Audio, part 3 of\n        MPEG-1, defined as\n        ISO/IEC-11172-3, is\n        the first standard audio compression technology. It only supports two\n        channels of audio. It is divided into three layers – Layer I,\n        II and III.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG-1 can compress video in 26:1 and audio in\n        6:1 ratio without significant notice in"),t("br"),e._v("\n        degradation of quality.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG-1 had following weaknesses which gave\n        birth to the next standard:"),t("br"),e._v("\n        • MPEG-1 had no standard support for\n        interlaced video."),t("br"),e._v("\n        • Audio compression is possible with only two channels."),t("br"),e._v("\n        • Only ‘4:2:0′ color space is supported."),t("br"),e._v("\n        • Not compatible with videos of high resolution.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG-2 is is the most popular standard for\n        video compression with huge range of applications. It was extended from\n        MPEG-1 with lots of improvements and feature\n        additions. It goes up to Part 11, Part 8 (10-bit video) being dropped.\n        MPEG-2 Part 3 ("),t("span",{staticClass:"caps"},[e._v("ISO")]),e._v("/IEC 13818-3) which describes Audio is\n        backward compatible with MPEG-1 Audio whereas\n        Part 7 describing AAC (Advanced Audio Coding)\n        is not. MPEG-2 adds Variable quantization and\n        Variable Bit Rate (VBR) to\n        MPEG-1 among other features. Since,\n        MPEG-2 has more complex algorithm for\n        encoding, MPEG-1 is slightly better for\n        compression of videos with low bit rates.\n        MPEG-2 can produce compression down to the bit\n        rate of around 3-15 Mbit/s. Any lower bit rate than this may introduce\n        noticeable impairments in the video.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Video")]),e._v(" is the technology in which moving visual images are\n        recorded, reproduced and/or broadcasted. An analog video is a video with\n        uninterrupted time varied signal. A digital video is composed of\n        sequence of still digital images. Such still images are called\n        "),t("b",[e._v("video frames")]),e._v(".\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Frame rate")]),e._v(" is the frequency in which consecutive still images\n        appear in a video. It is measured in fps ( frames per second).\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        An "),t("b",[e._v("interlaced video")]),e._v(" is a video with interwoven frames. Frames are\n        interwoven with fields. Each frame has two fields (or half-frames) – odd\n        fields and even fields.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Video Object (VO)")]),t("br"),e._v("\n        A video object is any arbitrarily shaped object, a rectangular frame or\n        the background scene of a video. A video object can be accessed and\n        manipulated by the user. A user can browse or seek the video objects\n        while accessing the video and cut or paste the video objects while\n        manipulating it.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Video Object Plane (VOP)")]),t("br"),e._v("\n        Video Object Plane is the instance of Video Object at a given time. It\n        may also be defined as time sample of video object. A\n        VOP of rectangular shape forms a conventional\n        video frame. A video is made up of such encoded VOPs.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Group of Video Object Planes (GOV)")]),t("br"),e._v("\n        A collection of video object planes is called Group of Video Object\n        Planes or GOV in short. GOVs are not mandatory\n        in videos. Random Access points in video bitstream is facilitated by the\n        points provided by GOVs. One of the reasons video objects are grouped\n        into GOV is that redundancy can not only be\n        removed from objects but from the whole group.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Video Object Layer (VOL)")]),t("br"),e._v("\n        A Video Object Layer is the representation of video objects in single or\n        multiple strata. A VOL with single stratum is\n        in non-scalable form whereas a VOL with\n        multiple strata is a scalable form.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG identifies frames into three kinds:"),t("br"),e._v("\n        1. I-frames"),t("br"),e._v("\n        2. P-frames"),t("br"),e._v("\n        3. B-frames\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Since P-frames and B-frames only store the changed information relative\n        to the other"),t("br"),e._v("\n        frames, they are also known as delta frames.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Open GOPs and Closed GOPs")]),t("br"),e._v("\n        Open GOPs are the GOPs which contain frames that can refer to the frames\n        from preceding or following GOPs. I contrast, delta frames in closed\n        GOPs can only refer to its own I-frames. Among open and closed GOPs of\n        same number of frames, open provides a little more compression since\n        open GOPs contain one less P-frame and one more B-frame than closed\n        GOPs. However open GOPs aren’t applicable to all\n        MPEG streams like mixed-angle or\n        multi-angle DVDs.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        A macroblock (MB) is a segment or component of\n        frame. 16×16 blocks of pixels are usually taken as a macroblock where as\n        8 by 8 pixels for advanced prediction mode. A still image or video frame\n        contains several non-overlapping macroblocks. Macroblocks are the basic\n        units of consideration in motion estimation and compensation. They are\n        also used used for motion vectors for predictive coding of frames, that\n        is for finding the difference between two frames. A series of\n        macroblocks forms a "),t("b",[e._v("slice")]),e._v(".\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Wavelet compression may also be employed in\n        MPEG-4 technology as a substitute to\n        DCT. Quantization of the frequency variations\n        is performed during the process of which many coefficients, generally\n        the coefficients with higher frequency are changed to zero. This\n        quantization is a lossy process, which means the inverse transformation\n        of the quantized matrix gives a similar matrix but not the original one,\n        however, this doesn’t significantly alter the image quality. Non-linear\n        quantization of Direct Cosine coefficients is also possible with\n        MPEG-4. MPEG-4\n        employs Twin Vector Quantization (VQF) which\n        considers time domain as one of the dimensions. The matrix of quantized\n        matrix is then itself compressed to obtain zeros on one corner of the\n        matrix. Zigzagging is performed beginning from the corner opposite to\n        where the zeros have been aligned. Zigzagging combines the coefficients\n        into a string. The redundant consecutive zeros from the string are\n        substituted with run-length codes which is the popular Run length\n        Encoding (RLE) algorithm. Huffman Coding is\n        then applied as a VLC to obtain smaller array\n        of"),t("br"),e._v(" numbers.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        H.264/MPEG-AVC\n        employs Context-Adaptive Binary Arithmetic Coding ("),t("span",{staticClass:"caps"},[e._v("CABAC")]),e._v(") and Context-adaptive Variable-length Coding ("),t("span",{staticClass:"caps"},[e._v("CAVLC")]),e._v(") for variable-length coding. CABAC is used\n        whenever higher compression is required and\n        CAVLC is used in slower playback devices to\n        increase the performance since it is a lower efficiency scheme. All\n        H.264 profiles support CAVLC whereas only\n        Baseline and Extended profiles do not support\n        CABAC. So\n        CAVLC support is seen everywhere in in all\n        kinds of decoders like Blu-ray and HD\n        DVD players.\n        CAVLC is also supposed to be a superior\n        technique than DivX, XviD and other MPEG-4\n        ASP codecs. Although\n        CAVLC may be used for coding transform\n        coefficients, Exponential-Golomb Coding is used to code other syntax\n        elements in the video stream.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Encoding of P-frames and encoding of B-frames are similar except that\n        P-frames refer to the information in the previous I-frame or P-frame and\n        B-frames refer to both the previous and next frames. As the first step,\n        the previous frame is reconstructed for P-frame encoding and both\n        previous and next frames are reconstructed for encoding of B-frames to\n        "),t("strong",[e._v("obtain reference frame")]),e._v(". The frame to be compressed is\n        then "),t("strong",[e._v("segmented into macroblocks")]),e._v(", say of block size 16\n        by 16 pixels. The reference frame is\n        "),t("strong",[e._v("searched for best match of the macroblocks")]),e._v(" for each\n        macroblocks of the frame being compressed. Exhaustive search, 2-D\n        Logarithmic search, or Three-Step Search (TSS)\n        may be used to optimize the search. For most of the macroblocks, an\n        exact match is found on the reference frame because two subsequent\n        frames have very less change in their blocks. Since some components move\n        in the frames, offset is calculated. A component may be"),t("br"),e._v("\n        moving 15 pixels to the right and 9 pixels up. Such\n        "),t("strong",[e._v("offset or shift is represented as ‘motion vector’")]),e._v(",\n        which is a two-dimensional vector. The offset and the motion vector is\n        frequently zero because most blocks in subsequent frames are consistent.\n        Offset value is not always enough to describe the change in frames\n        because sometimes not just the placement of macroblock changes but their\n        appearance too. This change can be measured by taking into account each\n        pixels of the two macroblocks and\n        "),t("strong",[e._v("finding the difference between corresponding pixels")]),e._v(".\n        The difference is computed as coefficient values which is then obtained\n        as string called "),t("strong",[e._v("residual")]),e._v(". This residual value\n        undergoes "),t("strong",[e._v("compression")]),e._v(". The residual value which is in\n        spatial domain is transformed using DCT of two\n        dimensions. The transformation coefficients are then quantized to reduce\n        their bit size, most of them quantized to zero. The quantized\n        coefficients are passed through entropy coding for further compression.\n        The final information is then combined with the motion vector along with\n        other information like frame types, etc. to obtain the final difference\n        known as "),t("strong",[e._v("prediction error")]),e._v(" between the frames.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        The above explained method is the most common method of motion\n        estimation indifferent video coding standards, also seen on\n        MPEG. This technique has been given several\n        names such as Block-Matching Algorithm (BMA)\n        or Block Matching Compensation (BMC). This\n        block-based method utilizes algorithms like Mean of Squared Error ("),t("span",{staticClass:"caps"},[e._v("MSE")]),e._v("), Matching Pel Count (MPC), Mean of Absolute\n        Difference (MAD), Sum of Absolute Difference\n        (SAD), etc. SAD is\n        used in MPEG technology to get the variation\n        of macroblock matches using polygons of modified blocks.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Older MPEG standards use fixed block size for\n        the motion estimation and compensation process. Newer standards like\n        MPEG-4 Part 2,\n        MPEG-4 AVC use\n        dynamically selected size of the blocks. This method is known as\n        variable block-size motion compensation ("),t("span",{staticClass:"caps"},[e._v("VBSMC")]),e._v("). This enables the encoder to use larger or smaller block size\n        whichever is more efficient. Larger block size can decrease the bits\n        required to represent the motion vector whereas smaller block size may\n        allow for more precise prediction error.\n        MPEG-4 even allows wavelet transforms instead\n        of DCT during the conversion of spatial\n        variations to"),t("br"),e._v("\n        functional variations.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        MPEG-1 is used for VCDs (Video CDs). The very\n        popular DVD videos are possible only because\n        of MPEG-2. The latest Blu-ray technology also\n        utilizes MPEG-2 Part 2 and H.264/"),t("span",{staticClass:"caps"},[e._v("MPEG")]),e._v("-4 AVC.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Newer standards of the MPEG family keep being\n        published to overcome the limitations of the older one. The latest\n        popular standard has many limitations. The current downsides of the\n        format have been listed below:"),t("br"),e._v("\n        • MPEG being a lossy compression doesn’t\n        preserve all minute data."),t("br"),e._v("\n        • The compression flow involves many encoding and transformation\n        algorithms which could be difficult to perceive and implement."),t("br"),e._v("\n        • AAC and OGG Vorbis\n        provide better audio encoding than MPEG audio\n        layers."),t("br"),e._v("\n        • MPEG is not the best file format for online\n        streaming and therefore is slowly"),t("br"),e._v("\n        being replaced by other file formats like\n        FLV (Flash Video)."),t("br"),e._v("\n        • Overlapped Block Motion Compensation (OBMC)\n        is not allowed in profiles in all parts of\n        MPEG-4 standards."),t("br"),e._v("\n        • MPEG-4 has a short header format."),t("br"),e._v("\n        • Initial parts of MPEG-4 couldn’t provide\n        significant improvement in bit-rate that caused users to be attracted to\n        other encoding formats."),t("br"),e._v("\n        • Since MPEG is a patented format, non-free\n        MPEG encoders and decoders may be subject to\n        royalty fee."),t("br"),e._v("\n        • End-users may have to bear the royalty when using free software like\n        VLC for encoding and decoding purposes.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("REFERENCES")])])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        William B. Pennebaker, Joan L. Mitchell, 1992.\n        JPEG: "),t("em",[e._v("Still Image Data Compression")]),e._v(" "),t("em",[e._v("Standard (Digital Multimedia Standards) (Digital Multimedia Standards\n          S.)")]),e._v(". 1st Edition. Springer.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Vasudev Bhaskaran, 1997.\n        "),t("em",[e._v("Image and Video Compression Standards: Algorithms and")]),e._v(" "),t("em",[e._v("Architectures (The Springer International Series in Engineering and\n          Computer Science)")]),e._v(". 2nd Edition. Springer.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Jerry D. Gibson, 1998.\n        "),t("em",[e._v("Digital Compression for Multimedia: Principles\n          "),t("span",{staticClass:"amp"},[e._v("&")]),e._v(" Standards")]),e._v(" "),t("em",[e._v("(The Morgan Kaufmann Series in Multimedia Information and\n          Systems)")]),e._v(". 1 Edition. Morgan Kaufmann.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        David Salomon, 2004. "),t("em",[e._v("Data Compression: The Complete Reference")]),e._v(".\n        3rd Edition. Springer.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Chad Fogg, 1996.\n        "),t("em",[e._v("MPEG Video Compression Standard (Digital\n          Multimedia Standards ")]),t("em",[e._v("Series)")]),e._v(". 1 Edition. Springer.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Wallace, Gregory K.,\n        "),t("em",[e._v("The JPEG Still Picture Compression\n          Standard, Communications of")]),e._v(" "),t("em",[e._v("the ACM")]),e._v(", April 1991 (Vol. 34, No. 4), pp. 30-44.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Neelamani, R., de Queiroz, R., Fan, Z., Dash, S.,\n        "),t("span",{staticClass:"amp"},[e._v("&")]),e._v(" Baraniuk, R.,\n        "),t("em",[e._v("JPEG compression")]),e._v(" "),t("em",[e._v("history estimation for color images")]),e._v(",\n        IEEE Trans. on Image Processing, June 2006\n        (Vol 15, No 6).\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Ja-Ling Wu, 2002. ‘MPEG-1 Coding Standard’,\n        "),t("em",[e._v("DSP (Digital Signal Processing)")]),e._v(".\n        [online via internal VLE] Communications and\n        Multimedia Library, Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/mpeg1/",title:"http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/mpeg1/",target:"_blank"}},[e._v("http://www.cmlab.csie.ntu.edu.tw/cml/dsp/training/coding/mpeg1/")]),e._v(">. [Accessed 17 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Prof. Tsuhan Chen, 1999. ‘MPEG Audio’, 18-796\n        ("),t("em",[e._v("Multimedia Communications")]),e._v("), [online via internal\n        VLE] Carneggie Melon University, Available at:\n        <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.ece.cmu.edu/~ece796/mpegaudio.pdf",title:"http://www.ece.cmu.edu/~ece796/mpegaudio.pdf",target:"_blank"}},[e._v("http://www.ece.cmu.edu/~ece796/mpegaudio.pdf")]),e._v(">. [Accessed 19 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Avila A., n.d. ‘History of MPEG’,\n        IS224 ("),t("em",[e._v("Strategic Computing and Communications Technlology")]),e._v("), [online via internal VLE] School of\n        Information Management and Systems, California, Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www2.sims.berkeley.edu/courses/is224/s99/GroupG/report1.html",title:"http://www2.sims.berkeley.edu/courses/is224/s99/GroupG/report1.html",target:"_blank"}},[e._v("http://www2.sims.berkeley.edu/courses/is224/s99/GroupG/report1.html")]),e._v(">. [Accessed 15 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Berkeley Design Technology, Inc., 2006.\n        "),t("em",[e._v("Introduction to Video Compression")]),e._v(", April 13 [online] Electronic\n        Engineering Times. Available at <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.eetimes.com/design/signal-processing-dsp/4013042/Introduction-to-video-compression",title:"http://www.eetimes.com/design/signal-processing-dsp/4013042/Introduction-to-video-compression",target:"_blank"}},[e._v("http://www.eetimes.com/design/signal-processing-dsp/4013042/Introduction-to-video-compression")]),e._v(">. [Accessed 16 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Michael Niedermayer, 2005. 15 reasons why\n        MPEG4 sucks.\n        "),t("em",[e._v("Liar of the Multimedia Guru")]),e._v(", [blog] 28 November, Available at:\n        <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://guru.multimedia.cx/15-reasons-why-mpeg4-sucks/",title:"http://guru.multimedia.cx/15-reasons-why-mpeg4-sucks/",target:"_blank"}},[e._v("http://guru.multimedia.cx/15-reasons-why-mpeg4-sucks/")]),e._v("> [Accessed 22 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        P.N. Tudor, December 1995.\n        MPEG-2 Video Compression,\n        "),t("em",[e._v("Electronics and Communication Engineering Journal")]),e._v(", [online],\n        Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.bbc.co.uk/rd/pubs/papers/paper_14/paper_14.shtml",title:"from archive.org",target:"_blank"}},[e._v("http://www.bbc.co.uk/rd/pubs/papers/paper_14/paper_14.shtml")]),e._v("> [Accessed 18 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Ebrahimi, T.,Horne, C., 2010.\n        "),t("em",[e._v("MPEG-4 Natural Video Coding – An\n          overview")]),e._v(", Swiss Federal Institute of Technology [online] 04 February, Available\n        at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://mpeg.chiariglione.org/tutorials/papers/icj-mpeg4-si/07-natural_video_paper/7-natural_video_paper.htm",title:"from archive.org",target:"_blank"}},[e._v("http://mpeg.chiariglione.org/tutorials/papers/icj-mpeg4-si/07-natural_video_paper/7-natural_video_paper.htm")]),e._v(">\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Bretl W., Fimoff M., 2000.\n        "),t("em",[e._v("MPEG2 Tutorial")]),e._v(" [online] 15 January,\n        Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.bretl.com/mpeghtml/MPEGindex.htm",title:"http://www.bretl.com/mpeghtml/MPEGindex.htm",target:"_blank"}},[e._v("http://www.bretl.com/mpeghtml/MPEGindex.htm")]),e._v("> [Accessed 18 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("Images")])])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        DePiero, F. W., 2011.\n        "),t("em",[e._v("2D DCT and\n          JPEG")]),e._v("\n        [image online] Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/https://courseware.ee.calpoly.edu/~fdepiero/STL/STL%20-%20Image%20-%202D%20DCT%20and%20JPEG.htm",title:"https://courseware.ee.calpoly.edu/~fdepiero/STL/STL%20-%20Image%20-%202D%20DCT%20and%20JPEG.htm",target:"_blank"}},[e._v("https://courseware.ee.calpoly.edu/~fdepiero/"),t("span",{staticClass:"caps"},[e._v("STL")]),e._v("/STL%20-%20Image%20-%202D%"),t("span",{staticClass:"caps"},[e._v("20DCT")]),e._v("%20and%20JPEG.htm")]),e._v(">\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Chan, G., 2010. "),t("em",[e._v("Towards Better Chroma Subsampling")]),e._v(" [image\n        online] Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://www.glennchan.info/articles/technical/chroma/chroma1.htm",title:"http://www.glennchan.info/articles/technical/chroma/chroma1.htm",target:"_blank"}},[e._v("http://www.glennchan.info/articles/technical/chroma/chroma1.htm")]),e._v("> [Accessed 14 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Luke, 2002. "),t("em",[e._v("Frame fields")]),e._v(" [image online] Available at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://neuron2.net/LVG/framefields.gif",title:"http://neuron2.net/LVG/framefields.gif",target:"_blank"}},[e._v("http://neuron2.net/LVG/framefields.gif")]),e._v("> [Accessed 14 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Apple, Inc. n.d. "),t("em",[e._v("Open and Closed GOPs")]),e._v(" [image online] Available\n        at: <"),t("a",{attrs:{href:"http://web.archive.org/web/20170930220816/http://documentation.apple.com/en/compressor/usermanual/Art/L00/L0006_IBBP.png",title:"http://documentation.apple.com/en/compressor/usermanual/Art/L00/L0006_IBBP.png",target:"_blank"}},[e._v("http://documentation.apple.com/en/compressor/usermanual/Art/L00/L0006_IBBP.png")]),e._v("> [Accessed 17 November 2011].\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[t("b",[e._v("ACKNOWLEDGEMENTS")]),t("br"),e._v("\n        I would like to show my gratitude to the contributors of Wikipedia and\n        the World Wide Web. Also, huge thanks goes to the lecturer Mr. Ayush\n        Subedi and my friend Binayak Upadhaya.\n      ")])}],!1,null,"f0c4a2de",null);n.default=component.exports}}]);