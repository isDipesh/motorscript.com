(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{170:function(e,n,t){"use strict";var o={props:["title","published","updated","archived"],head:function(){return{title:this.title}}},r=t(8),component=Object(r.a)(o,(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",[t("h1",{staticClass:"title",attrs:{itemprop:"name headline"}},[e._v(e._s(e.title))]),e._v(" "),t("div",{staticClass:"time"},[e._v("Published: "),t("time",{attrs:{itemprop:"datePublished"}},[e._v(e._s(e.published))])]),e._v(" "),e.updated?t("div",{staticClass:"time"},[e._v("Updated: "),t("time",{attrs:{itemprop:"dateModified"}},[e._v(e._s(e.updated))])]):e._e(),e._v(" "),e.archived?t("div",{staticClass:"block"},[e._v("Note: This is an archived post. Information may not be relevant now.")]):e._e()])}),[],!1,null,null,null);n.a=component.exports},226:function(e,n,t){"use strict";t.r(n);var o=t(170),r={mixins:[o.a],components:{BlogTitle:o.a}},d=t(8),component=Object(d.a)(r,(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("article",{attrs:{itemprop:"blogPost",itemscope:"",itemtype:"https://schema.org/BlogPosting"}},[t("BlogTitle",{attrs:{title:"Using Google Cloud Vision to Build ML Training Dataset",published:"27 Oct 2011"}}),e._v(" "),t("div",{directives:[{name:"highlight",rawName:"v-highlight"}],staticClass:"content",attrs:{itemprop:"articleBody"}},[e._m(0),e._v(" "),t("p",[e._v("\n        The code provided by Google needed a fix nonetheless since it hits API\n        with same request multiple times which would increase our cost. API\n        requests were separately made for Page, Paragraph and Word detection\n        while the same could be done by reusing the response from single\n        request. However, we are only using Paragraph detection to find the\n        bounding box for MRZ. The modified script also optionally uses Redis to\n        store the serialized vertices of the bounds and reuses them if the same\n        image needs to processed again.\n      ")]),e._v(" "),t("p",[e._v("\n        The script is only used for building training data but not for\n        production. The output of this script is used for training for pattern\n        matching which is then used in production. This same technique can be\n        used to detect other sections in passport or other document types. Just\n        adjust the index in the assignment of `target_bound` variable.\n      ")]),e._v(" "),e._m(1),e._v(" "),e._m(2),e._v(" "),e._m(3)])],1)}),[function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        While building an OCR system for passports, I needed to crop out MRZ\n        (Machine Readable Zone) from passports. In order to build the training\n        set, I needed ground truth values. MRZ of the passports needed to be\n        annotated. The two popular ways for this were either labellilng manually\n        using tools like LabelImg or outsourcing using something like Amazon\n        Mechanical Turk. I used Google Cloud Vision API rather because I knew it\n        does well with labelling such things and returning bounds within such\n        documents. The following script uses document text annotation example\n        from\n        "),t("a",{attrs:{href:"https://cloud.google.com/vision/docs/fulltext-annotations",target:"_blank",rel:"noreferer noopener"}},[e._v("https://cloud.google.com/vision/docs/fulltext-annotations")]),e._v("\n        and improves over it.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("p",[e._v("\n        Before running the script, you need to download your GCP credentails set\n        the path to the credentials json as\n        "),t("code",[e._v("GOOGLE_APPLICATION_CREDENTIALS")]),e._v(" environment variable.\n      ")])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("pre",{staticClass:"language-bash command-line",attrs:{"data-prompt":"$"}},[t("code",[e._v('export GOOGLE_APPLICATION_CREDENTIALS="/home/user/Downloads/creditials_file_name.json"')])])},function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("pre",{staticClass:"language-python"},[t("code",[e._v("import argparse\nfrom enum import Enum\nimport io\n\nfrom google.cloud import vision\nfrom google.cloud.vision import types\nfrom google.cloud.vision_v1.types import BoundingPoly\nfrom PIL import Image, ImageDraw\n\nuse_redis = False\ntry:\n    import redis\n\n    try:\n        cache = redis.Redis(host='localhost', port=6379, db=0)\n        # execute a command to test connection\n        cache.client_list()\n        use_redis = True\n    except redis.exceptions.ConnectionError:\n        pass\nexcept ImportError:\n    pass\n\n\nclass FeatureType(Enum):\n    PAGE = 1\n    BLOCK = 2\n    PARA = 3\n    WORD = 4\n    SYMBOL = 5\n\n\ndef draw_boxes(image, bounds, color):\n    \"\"\"Draw a border around the image using the hints in the vector list.\"\"\"\n    draw = ImageDraw.Draw(image)\n\n    for bound in bounds:\n        draw.polygon([\n            bound.vertices[0].x - 10, bound.vertices[0].y,\n            bound.vertices[1].x + 10, bound.vertices[1].y,\n            bound.vertices[2].x + 5, bound.vertices[2].y,\n            bound.vertices[3].x, bound.vertices[3].y], None, color)\n    return image\n\n\ndef get_document_bounds(document, feature):\n    \"\"\"Returns document bounds given an image.\"\"\"\n\n    bounds = []\n\n    # Collect specified feature bounds by enumerating all document features\n    for page in document.pages:\n        for block in page.blocks:\n            for paragraph in block.paragraphs:\n                for word in paragraph.words:\n                    for symbol in word.symbols:\n                        if feature == FeatureType.SYMBOL:\n                            bounds.append(symbol.bounding_box)\n\n                    if feature == FeatureType.WORD:\n                        bounds.append(word.bounding_box)\n\n                if feature == FeatureType.PARA:\n                    bounds.append(paragraph.bounding_box)\n\n            if feature == FeatureType.BLOCK:\n                bounds.append(block.bounding_box)\n\n        if feature == FeatureType.PAGE:\n            # noinspection PyUnboundLocalVariable\n            bounds.append(block.bounding_box)\n\n    # The list `bounds` contains the coordinates of the bounding boxes.\n    return bounds\n\n\ndef render_doc_text(directory):\n    from os import listdir, path\n\n    dir_content = [f for f in listdir(directory) if not '_bounded' in f]\n    files = [f for f in dir_content if path.isfile(path.join(directory, f))]\n    for file_name in files:\n        file = path.join(directory, file_name)\n        client = vision.ImageAnnotatorClient()\n        with io.open(file, 'rb') as image_file:\n            content = image_file.read()\n        target_bound_str = cache.get(file) if use_redis else None\n\n        if target_bound_str:\n            target_bound = BoundingPoly().FromString(target_bound_str)\n        else:\n            # noinspection PyUnresolvedReferences\n            image_type = types.Image(content=content)\n            response = client.document_text_detection(image=image_type)\n            document = response.full_text_annotation\n\n            bounds = get_document_bounds(document, FeatureType.PARA)\n\n            if len(bounds):\n                target_bound = bounds[-1]\n                target_bound_str = target_bound.SerializeToString()\n\n                if use_redis:\n                    cache.set(file, target_bound_str)\n\n        # target_bound_str exists implies target_bound exists\n        if target_bound_str:\n            image = Image.open(file)\n            # noinspection PyUnboundLocalVariable\n            draw_boxes(image, [target_bound], 'green')\n            sans_ext, ext = path.splitext(file)\n            out_name = sans_ext + '_bounded' + ext\n            image.save(out_name)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('directory', help='Directory where image files are.')\n    args = parser.parse_args()\n    parser = argparse.ArgumentParser()\n\n    render_doc_text(args.directory)\n")])])}],!1,null,null,null);n.default=component.exports}}]);